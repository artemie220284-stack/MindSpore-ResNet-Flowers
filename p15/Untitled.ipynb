{
 "cells": [
  {
   "cell_type": "code",
   "id": "2f44560f-1c73-4040-940c-721eec5810fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:04.892230853Z",
     "start_time": "2025-12-29T04:38:04.861281688Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# è®¾ç½®CUDA_HOMEçŽ¯å¢ƒå˜é‡\n",
    "conda_prefix = os.environ.get('CONDA_PREFIX')\n",
    "if conda_prefix:\n",
    "    os.environ['CUDA_HOME'] = conda_prefix\n",
    "    os.environ['LD_LIBRARY_PATH'] = f\"{conda_prefix}/lib:{os.environ.get('LD_LIBRARY_PATH', '')}\"\n",
    "    \n",
    "    print(f\"å·²è®¾ç½®çŽ¯å¢ƒå˜é‡:\")\n",
    "    print(f\"  CUDA_HOME: {os.environ['CUDA_HOME']}\")\n",
    "    print(f\"  CONDA_PREFIX: {conda_prefix}\")\n",
    "    \n",
    "    # éªŒè¯libcuda.soæ˜¯å¦å­˜åœ¨\n",
    "    libcuda_path = f\"{conda_prefix}/lib/libcuda.so\"\n",
    "    if os.path.exists(libcuda_path):\n",
    "        print(f\"  âœ“ libcuda.so: æ‰¾åˆ° ({libcuda_path})\")\n",
    "    else:\n",
    "        print(f\"  âœ— libcuda.so: åœ¨condaçŽ¯å¢ƒä¸­æœªæ‰¾åˆ°\")\n",
    "        \n",
    "        # åˆ›å»ºç¬¦å·é“¾æŽ¥åˆ°ç³»ç»Ÿlibcuda\n",
    "        system_libcuda = \"/usr/lib/x86_64-linux-gnu/libcuda.so\"\n",
    "        if os.path.exists(system_libcuda):\n",
    "            os.system(f\"ln -sf {system_libcuda} {conda_prefix}/lib/libcuda.so\")\n",
    "            print(f\"  âœ“ å·²åˆ›å»ºç¬¦å·é“¾æŽ¥: {conda_prefix}/lib/libcuda.so -> {system_libcuda}\")\n",
    "        else:\n",
    "            print(f\"  âš  ç³»ç»Ÿlibcuda.soä¹Ÿæœªæ‰¾åˆ°\")\n",
    "else:\n",
    "    print(\"CONDA_PREFIXæœªè®¾ç½®ï¼Œè¯·å…ˆæ¿€æ´»condaçŽ¯å¢ƒ\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²è®¾ç½®çŽ¯å¢ƒå˜é‡:\n",
      "  CUDA_HOME: /home/y/anaconda3/envs/mindspore\n",
      "  CONDA_PREFIX: /home/y/anaconda3/envs/mindspore\n",
      "  âœ“ libcuda.so: æ‰¾åˆ° (/home/y/anaconda3/envs/mindspore/lib/libcuda.so)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "c0d1521a-3760-42b2-b90d-2c3896ba5aae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:04.974566227Z",
     "start_time": "2025-12-29T04:38:04.893362997Z"
    }
   },
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')  # ä½¿ç”¨éžäº¤äº’å¼åŽç«¯ï¼Œé¿å…GUIé—®é¢˜\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# è®¾ç½®ä¸­æ–‡å­—ä½“\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# ============================================================\n",
    "# å…ˆå¯¼å…¥MindSporeç›¸å…³æ¨¡å—\n",
    "# ============================================================\n",
    "try:\n",
    "    import mindspore as ms\n",
    "    from mindspore import context, Tensor, Model, load_checkpoint, save_checkpoint, Parameter\n",
    "    import mindspore.nn as nn\n",
    "    import mindspore.ops as ops\n",
    "    from mindspore.train.callback import LossMonitor, TimeMonitor, ModelCheckpoint, CheckpointConfig\n",
    "    from mindspore import dataset as ds\n",
    "    import mindspore.dataset.vision as vision\n",
    "    import mindspore.dataset.transforms as transforms\n",
    "    \n",
    "    print(f\"âœ“ MindSporeç‰ˆæœ¬: {ms.__version__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âœ— MindSporeå¯¼å…¥å¤±è´¥: {e}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# ============================================================\n",
    "# å¼ºåˆ¶ä½¿ç”¨åŠ¨æ€å›¾æ¨¡å¼\n",
    "# ============================================================\n",
    "print(\"=\"*60)\n",
    "print(\"å¼ºåˆ¶ä½¿ç”¨åŠ¨æ€å›¾æ¨¡å¼\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # å¼ºåˆ¶è®¾ç½®ä¸ºåŠ¨æ€å›¾æ¨¡å¼\n",
    "    context.set_context(mode=context.PYNATIVE_MODE, device_target='GPU')\n",
    "    print(f\"âœ“ è¿è¡Œæ¨¡å¼: PYNATIVE_MODE (åŠ¨æ€å›¾)\")\n",
    "    print(f\"âœ“ è®¾å¤‡: GPU\")\n",
    "\n",
    "    # éªŒè¯è®¾ç½®\n",
    "    current_mode = context.get_context(\"mode\")\n",
    "    mode_str = \"åŠ¨æ€å›¾\" if current_mode == context.PYNATIVE_MODE else \"é™æ€å›¾\"\n",
    "    print(f\"âœ“ ç¡®è®¤æ¨¡å¼: {mode_str}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âš  GPUè®¾ç½®å¤±è´¥: {e}\")\n",
    "    # å›žé€€åˆ°CPU\n",
    "    context.set_context(mode=context.PYNATIVE_MODE, device_target='CPU')\n",
    "    print(f\"âœ“ å›žé€€åˆ°CPU (åŠ¨æ€å›¾æ¨¡å¼)\")\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ MindSporeç‰ˆæœ¬: 2.2.0\n",
      "============================================================\n",
      "å¼ºåˆ¶ä½¿ç”¨åŠ¨æ€å›¾æ¨¡å¼\n",
      "============================================================\n",
      "âœ“ è¿è¡Œæ¨¡å¼: PYNATIVE_MODE (åŠ¨æ€å›¾)\n",
      "âœ“ è®¾å¤‡: GPU\n",
      "âœ“ ç¡®è®¤æ¨¡å¼: åŠ¨æ€å›¾\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "38d962e9-2a3f-4a11-aa0e-627463cc56b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:05.026404478Z",
     "start_time": "2025-12-29T04:38:04.987272618Z"
    }
   },
   "source": [
    "class Config:\n",
    "    # è·¯å¾„é…ç½®\n",
    "    data_dir = \"flowers\"  # æ•°æ®ç›®å½•\n",
    "    model_path = \"best_flower_model.ckpt\"  # æ¨¡åž‹ä¿å­˜è·¯å¾„\n",
    "    model_path1 = \"best_flower_model1.ckpt\"\n",
    "    metrics_path1 = \"training_metrics1.npy\"  # æŒ‡æ ‡ä¿å­˜è·¯å¾„\n",
    "    metrics_path = \"training_metrics.npy\"\n",
    "    # ç±»åˆ«ä¿¡æ¯\n",
    "    class_names = [\"daisy\", \"dandelion\", \"rose\", \"sunflower\", \"tulip\"]\n",
    "    num_classes = len(class_names)\n",
    "    \n",
    "    # è®­ç»ƒå‚æ•°\n",
    "    batch_size = 64\n",
    "    epochs = 25\n",
    "    learning_rate = 5e-4\n",
    "    image_size = (224, 224)\n",
    "    \n",
    "    # æ•°æ®é›†åˆ†å‰²\n",
    "    train_ratio = 0.9  # è®­ç»ƒé›†æ¯”ä¾‹\n",
    "    \n",
    "    # åŠ¨é™æ€å›¾ç›¸å…³é…ç½®\n",
    "    @property\n",
    "    def is_graph_mode(self):\n",
    "        \"\"\"æ£€æŸ¥æ˜¯å¦åœ¨é™æ€å›¾æ¨¡å¼ä¸‹è¿è¡Œ\"\"\"\n",
    "        return context.get_context('mode') == context.GRAPH_MODE\n",
    "    \n",
    "    @property\n",
    "    def is_pynative_mode(self):\n",
    "        \"\"\"æ£€æŸ¥æ˜¯å¦åœ¨åŠ¨æ€å›¾æ¨¡å¼ä¸‹è¿è¡Œ\"\"\"\n",
    "        return context.get_context('mode') == context.PYNATIVE_MODE\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"è¿è¡Œæ¨¡å¼ç¡®è®¤\")\n",
    "print(\"=\"*60)\n",
    "print(f\"å½“å‰æ¨¡å¼: {'é™æ€å›¾ (GRAPH_MODE)' if config.is_graph_mode else 'åŠ¨æ€å›¾ (PYNATIVE_MODE)'}\")\n",
    "print(f\"è®¾å¤‡: {context.get_context('device_target')}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "è¿è¡Œæ¨¡å¼ç¡®è®¤\n",
      "============================================================\n",
      "å½“å‰æ¨¡å¼: åŠ¨æ€å›¾ (PYNATIVE_MODE)\n",
      "è®¾å¤‡: GPU\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:05.079229237Z",
     "start_time": "2025-12-29T04:38:05.028057448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_dataset():\n",
    "    \"\"\"å‡†å¤‡å’Œé¢„å¤„ç†æ•°æ®é›†\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"æ•°æ®å‡†å¤‡\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # æ£€æŸ¥æ•°æ®ç›®å½•\n",
    "    if not os.path.exists(config.data_dir):\n",
    "        print(f\"âœ— æ•°æ®ç›®å½•ä¸å­˜åœ¨: {config.data_dir}\")\n",
    "        print(\"è¯·ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹åˆ›å»º 'flowers' æ–‡ä»¶å¤¹ï¼Œå¹¶åŒ…å«ä»¥ä¸‹å­ç›®å½•ï¼š\")\n",
    "        print(\"  flowers/train/  - è®­ç»ƒå›¾åƒ\")\n",
    "        print(\"  flowers/test/   - æµ‹è¯•å›¾åƒ\")\n",
    "        print(\"æ¯ä¸ªå­ç›®å½•ä¸‹åº”æœ‰5ä¸ªæ–‡ä»¶å¤¹: daisy, dandelion, rose, sunflower, tulip\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    train_dir = os.path.join(config.data_dir, \"train\")\n",
    "    test_dir = os.path.join(config.data_dir, \"test\")\n",
    "\n",
    "    if not os.path.exists(train_dir) or not os.path.exists(test_dir):\n",
    "        print(\"âœ— æœªæ‰¾åˆ° train/ æˆ– test/ å­ç›®å½•\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # ç»Ÿè®¡æ•°æ®\n",
    "    def count_images(folder):\n",
    "        count = 0\n",
    "        for root, dirs, files in os.walk(folder):\n",
    "            count += len([f for f in files if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        return count\n",
    "\n",
    "    train_count = count_images(train_dir)\n",
    "    test_count = count_images(test_dir)\n",
    "\n",
    "    print(f\"âœ“ è®­ç»ƒé›†å›¾åƒ: {train_count} å¼ \")\n",
    "    print(f\"âœ“ æµ‹è¯•é›†å›¾åƒ: {test_count} å¼ \")\n",
    "    print(f\"âœ“ ç±»åˆ«æ•°é‡: {config.num_classes}\")\n",
    "    print(f\"âœ“ ç±»åˆ«åç§°: {config.class_names}\")\n",
    "\n",
    "    # å®šä¹‰æ•°æ®å˜æ¢ - å¢žå¼ºç‰ˆæœ¬\n",
    "    train_transform = [\n",
    "        vision.Decode(),  # å…³é”®ï¼šå¿…é¡»å…ˆè§£ç å›¾åƒ\n",
    "\n",
    "        # =============== å¢žå¼ºçš„æ•°æ®å¢žå¼º ===============\n",
    "        # 1. éšæœºè°ƒæ•´å¤§å°å’Œè£å‰ª\n",
    "        vision.RandomResizedCrop(\n",
    "            size=config.image_size,\n",
    "            scale=(0.7, 1.0),  # æ›´å®½çš„å°ºåº¦èŒƒå›´\n",
    "            ratio=(0.75, 1.33)  # æ›´å®½çš„é•¿å®½æ¯”\n",
    "        ),\n",
    "\n",
    "        # 2. éšæœºç¿»è½¬\n",
    "        vision.RandomHorizontalFlip(prob=0.5),\n",
    "        # vision.RandomVerticalFlip(prob=0.2),  # å¦‚æžœæ”¯æŒåˆ™å–æ¶ˆæ³¨é‡Š\n",
    "\n",
    "        # 3. é¢œè‰²å¢žå¼º\n",
    "        vision.RandomColorAdjust(\n",
    "            brightness=0.4,      # å¢žåŠ äº®åº¦è°ƒæ•´èŒƒå›´ (0.2 â†’ 0.4)\n",
    "            contrast=0.4,        # å¢žåŠ å¯¹æ¯”åº¦è°ƒæ•´èŒƒå›´ (0.2 â†’ 0.4)\n",
    "            saturation=0.4,      # å¢žåŠ é¥±å’Œåº¦è°ƒæ•´èŒƒå›´ (0.2 â†’ 0.4)\n",
    "            # hue=0.1           # è‰²è°ƒè°ƒæ•´ï¼ˆå¦‚æžœæ”¯æŒï¼‰\n",
    "        ),\n",
    "\n",
    "        # 4. å‡ ä½•å˜æ¢\n",
    "        vision.RandomRotation(degrees=30),  # å¢žåŠ æ—‹è½¬è§’åº¦ (15 â†’ 30)\n",
    "\n",
    "        # 5. éšæœºè£å‰ª+è°ƒæ•´å¤§å°ç»„åˆ\n",
    "        vision.Resize((int(config.image_size[0]*1.2), int(config.image_size[1]*1.2))),  # å…ˆæ”¾å¤§\n",
    "        vision.RandomCrop(size=config.image_size),  # éšæœºè£å‰ªå›žåŽŸå°ºå¯¸\n",
    "\n",
    "        # 6. æ·»åŠ éšæœºè£å‰ªå¢žå¼º\n",
    "        vision.RandomCrop(size=(int(config.image_size[0]*0.85), int(config.image_size[1]*0.85))),\n",
    "        vision.Resize(config.image_size),\n",
    "\n",
    "        vision.ToTensor(),\n",
    "        vision.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], is_hwc=False)\n",
    "    ]\n",
    "\n",
    "    # æ›´ç®€å•çš„æµ‹è¯•å˜æ¢\n",
    "    test_transform = [\n",
    "        vision.Decode(),\n",
    "        vision.Resize(config.image_size),\n",
    "        vision.ToTensor(),\n",
    "        vision.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], is_hwc=False)\n",
    "    ]\n",
    "\n",
    "    # åˆ›å»ºMindSporeæ•°æ®é›†\n",
    "    def create_mindspore_dataset(data_path, transform, shuffle=True):\n",
    "        dataset = ds.ImageFolderDataset(\n",
    "            data_path,\n",
    "            shuffle=shuffle,\n",
    "            extensions=[\".jpg\", \".jpeg\", \".png\", \".JPG\", \".JPEG\", \".PNG\"]\n",
    "        )\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            operations=transform,\n",
    "            input_columns=\"image\"\n",
    "        )\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            operations=transforms.TypeCast(ms.int32),\n",
    "            input_columns=\"label\"\n",
    "        )\n",
    "\n",
    "        dataset = dataset.batch(config.batch_size, drop_remainder=False)\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    # åˆ›å»ºæ•°æ®é›†\n",
    "    print(\"åˆ›å»ºè®­ç»ƒæ•°æ®é›†...\")\n",
    "    train_dataset = create_mindspore_dataset(train_dir, train_transform, shuffle=True)\n",
    "    print(\"åˆ›å»ºæµ‹è¯•æ•°æ®é›†...\")\n",
    "    test_dataset = create_mindspore_dataset(test_dir, test_transform, shuffle=False)\n",
    "\n",
    "    print(f\"âœ“ è®­ç»ƒæ‰¹æ¬¡æ•°: {train_dataset.get_dataset_size()}\")\n",
    "    print(f\"âœ“ æµ‹è¯•æ‰¹æ¬¡æ•°: {test_dataset.get_dataset_size()}\")\n",
    "\n",
    "    # æµ‹è¯•æ•°æ®é›†æ˜¯å¦èƒ½æ­£å¸¸è¯»å–\n",
    "    print(\"\\næµ‹è¯•æ•°æ®è¯»å–...\")\n",
    "    try:\n",
    "        test_iter = train_dataset.create_tuple_iterator()\n",
    "        test_images, test_labels = next(test_iter)\n",
    "        print(f\"âœ“ æ•°æ®è¯»å–æµ‹è¯•é€šè¿‡\")\n",
    "        print(f\"  å›¾åƒå½¢çŠ¶: {test_images.shape}\")\n",
    "        print(f\"  æ ‡ç­¾å½¢çŠ¶: {test_labels.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— æ•°æ®è¯»å–æµ‹è¯•å¤±è´¥: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # ç»Ÿè®¡ç±»åˆ«åˆ†å¸ƒ\n",
    "    class_counts = {}\n",
    "    for class_name in config.class_names:\n",
    "        class_dir = os.path.join(train_dir, class_name)\n",
    "        if os.path.exists(class_dir):\n",
    "            count = len([f for f in os.listdir(class_dir)\n",
    "                        if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "            class_counts[class_name] = count\n",
    "        else:\n",
    "            class_counts[class_name] = 0\n",
    "\n",
    "    return train_dataset, test_dataset, class_counts"
   ],
   "id": "115bcaef7c863992",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:05.202164086Z",
     "start_time": "2025-12-29T04:38:05.128467157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ResidualBlock(nn.Cell):\n",
    "    \"\"\"æ®‹å·®å—\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels, out_channels,\n",
    "            kernel_size=3, stride=stride,\n",
    "            pad_mode='pad', padding=1,\n",
    "            has_bias=False\n",
    "        )\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            out_channels, out_channels,\n",
    "            kernel_size=3, stride=1,\n",
    "            pad_mode='pad', padding=1,\n",
    "            has_bias=False\n",
    "        )\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # ä¸‹é‡‡æ ·å±‚ï¼ˆå¦‚æžœéœ€è¦ï¼‰\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.SequentialCell([\n",
    "                nn.Conv2d(\n",
    "                    in_channels, out_channels,\n",
    "                    kernel_size=1, stride=stride,\n",
    "                    pad_mode='valid',\n",
    "                    has_bias=False\n",
    "                ),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            ])\n",
    "\n",
    "    def construct(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n"
   ],
   "id": "df89b4360028afce",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:05.416843402Z",
     "start_time": "2025-12-29T04:38:05.369650245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FocalLoss(nn.Cell):\n",
    "    \"\"\"Focal Losså®žçŽ°\"\"\"\n",
    "    def __init__(self, gamma=2.0, alpha=None, reduction='mean'):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "        self.softmax = nn.Softmax(axis=1)\n",
    "        self.onehot = nn.OneHot(depth=config.num_classes)\n",
    "\n",
    "    def construct(self, inputs, targets):\n",
    "        # è®¡ç®—softmaxæ¦‚çŽ‡\n",
    "        probs = self.softmax(inputs)\n",
    "\n",
    "        # å°†targetsè½¬æ¢ä¸ºone-hotç¼–ç \n",
    "        targets_onehot = self.onehot(targets)\n",
    "\n",
    "        # è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„ç±»åˆ«æ¦‚çŽ‡\n",
    "        class_probs = ops.reduce_sum(probs * targets_onehot, axis=1)\n",
    "\n",
    "        # è®¡ç®—è°ƒåˆ¶å› å­ (1 - pt)^gamma\n",
    "        modulating_factor = ops.pow(1 - class_probs, self.gamma)\n",
    "\n",
    "        # è®¡ç®—äº¤å‰ç†µ\n",
    "        ce_loss = -ops.log(class_probs + 1e-8)\n",
    "\n",
    "        # è®¡ç®—focal loss\n",
    "        focal_loss = modulating_factor * ce_loss\n",
    "\n",
    "        # åº”ç”¨ç±»åˆ«æƒé‡ï¼ˆå¦‚æžœæä¾›ï¼‰\n",
    "        if self.alpha is not None:\n",
    "            alpha_factor = ops.gather(self.alpha, targets, 0)\n",
    "            focal_loss = alpha_factor * focal_loss\n",
    "\n",
    "        # æ ¹æ®reductionå‚æ•°è¿”å›žç»“æžœ\n",
    "        if self.reduction == 'mean':\n",
    "            return ops.mean(focal_loss)\n",
    "        elif self.reduction == 'sum':\n",
    "            return ops.sum(focal_loss)\n",
    "        else:\n",
    "            return focal_loss"
   ],
   "id": "cab4afc8849ad28e",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:05.466296450Z",
     "start_time": "2025-12-29T04:38:05.417578086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_dynamic_mode(model, train_dataset, test_dataset, class_counts=None):\n",
    "    \"\"\"\n",
    "    åŠ¨æ€å›¾æ¨¡å¼è®­ç»ƒå‡½æ•°\n",
    "    åˆ©ç”¨åŠ¨æ€å›¾ä¼˜åŠ¿è¿›è¡Œå®žæ—¶ç›‘æŽ§å’Œè°ƒæ•´\n",
    "    \"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸš€ åŠ¨æ€å›¾è®­ç»ƒæ¨¡å¼å¯åŠ¨\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # æ ¹æ®ç±»åˆ«åˆ†å¸ƒè®¡ç®—æƒé‡\n",
    "    if class_counts:\n",
    "        counts = np.array([class_counts.get(cls, 1) for cls in config.class_names])\n",
    "        counts = np.maximum(counts, 1)\n",
    "        weights = 1.0 / counts\n",
    "        weights = (weights - weights.min()) / (weights.max() - weights.min() + 1e-8)\n",
    "        weights = weights * 2 + 1  # æ˜ å°„åˆ°1-3\n",
    "        class_weights = Tensor(weights.astype(np.float32))\n",
    "\n",
    "        print(\"ç±»åˆ«æƒé‡ï¼ˆåŸºäºŽæ ·æœ¬åˆ†å¸ƒï¼‰:\")\n",
    "        for i, cls in enumerate(config.class_names):\n",
    "            print(f\"  {cls}: {weights[i]:.3f}\")\n",
    "    else:\n",
    "        class_weights = Tensor(np.ones(config.num_classes).astype(np.float32))\n",
    "\n",
    "    # ä½¿ç”¨Focal Loss\n",
    "    loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "    # loss_fn = FocalLoss(gamma=2.0, alpha=class_weights, reduction='mean')\n",
    "\n",
    "    # ä¼˜åŒ–å™¨\n",
    "    # optimizer = nn.Adam(\n",
    "    #     model.trainable_params(),\n",
    "    #     learning_rate=config.learning_rate,\n",
    "    #     weight_decay=1e-4\n",
    "    # )\n",
    "    # åˆ›å»ºå­¦ä¹ çŽ‡åˆ—è¡¨ï¼ˆæ‰‹åŠ¨å®žçŽ°warmupå’Œè¡°å‡ï¼‰\n",
    "    learning_rate = 0.001  # ä½¿ç”¨å›ºå®šçš„å­¦ä¹ çŽ‡\n",
    "\n",
    "    optimizer = nn.Adam(\n",
    "        model.trainable_params(),\n",
    "        learning_rate=learning_rate,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        eps=1e-08,\n",
    "        weight_decay=0.0001  # é€‚ä¸­çš„æƒé‡è¡°å‡\n",
    "    )\n",
    "\n",
    "    print(f\"  å›ºå®šå­¦ä¹ çŽ‡: {learning_rate}\")\n",
    "    print(f\"  Weight decay: 0.0001\")\n",
    "    print(f\"  Beta1: 0.9, Beta2: 0.999\")\n",
    "\n",
    "    # åˆ›å»ºè®­ç»ƒç½‘ç»œï¼ˆä½¿ç”¨MindSporeæ ‡å‡†æ–¹å¼ï¼‰\n",
    "    net_with_loss = nn.WithLossCell(model, loss_fn)\n",
    "    train_net = nn.TrainOneStepCell(net_with_loss, optimizer)\n",
    "\n",
    "    # è®°å½•è®­ç»ƒåŽ†å²\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': [],\n",
    "        'learning_rates': [config.learning_rate]\n",
    "    }\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "\n",
    "    # åŠ¨æ€å›¾è®­ç»ƒå¾ªçŽ¯\n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{config.epochs} [åŠ¨æ€å›¾æ¨¡å¼]\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        # åŠ¨æ€å­¦ä¹ çŽ‡è°ƒæ•´\n",
    "        if epoch > 0 and epoch % 8 == 0:\n",
    "            new_lr = config.learning_rate * (0.8 ** (epoch // 8))\n",
    "            # æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ çŽ‡\n",
    "            optimizer.learning_rate = Parameter(Tensor(new_lr, ms.float32))\n",
    "            print(f\"ðŸ“‰ åŠ¨æ€è°ƒæ•´å­¦ä¹ çŽ‡: {new_lr:.6f}\")\n",
    "            history['learning_rates'].append(new_lr)\n",
    "\n",
    "        # è®­ç»ƒé˜¶æ®µ\n",
    "        model.set_train()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for batch_idx, (images, labels) in enumerate(train_dataset.create_tuple_iterator()):\n",
    "            # ä½¿ç”¨train_netè¿›è¡Œå‰å‘ä¼ æ’­ã€æŸå¤±è®¡ç®—å’Œä¼˜åŒ–å™¨æ›´æ–°\n",
    "            loss = train_net(images, labels)\n",
    "\n",
    "            # èŽ·å–æ¨¡åž‹è¾“å‡ºç”¨äºŽè®¡ç®—å‡†ç¡®çŽ‡\n",
    "            outputs = model(images)\n",
    "\n",
    "            # è®¡ç®—å‡†ç¡®çŽ‡\n",
    "            predictions = ops.Argmax(output_type=ms.int32)(outputs)\n",
    "            batch_correct = (predictions.asnumpy() == labels.asnumpy()).sum()\n",
    "            batch_total = labels.shape[0]\n",
    "\n",
    "            correct += batch_correct\n",
    "            total += batch_total\n",
    "            epoch_loss += loss.asnumpy()\n",
    "\n",
    "            # è¿›åº¦æ˜¾ç¤º\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                current_acc = batch_correct / batch_total if batch_total > 0 else 0\n",
    "                print(f\"  Batch {batch_idx+1} | Loss: {loss.asnumpy():.4f} | Acc: {current_acc:.4f}\")\n",
    "\n",
    "        # è®¡ç®—epochæŒ‡æ ‡\n",
    "        train_loss = epoch_loss / train_dataset.get_dataset_size()\n",
    "        train_acc = correct / total\n",
    "\n",
    "        # éªŒè¯é˜¶æ®µ\n",
    "        val_loss, val_acc = evaluate_dynamic(model, test_dataset, loss_fn)\n",
    "\n",
    "        # è®°å½•åŽ†å²\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        # æ˜¾ç¤ºç»“æžœ\n",
    "        print(f\"\\nðŸ“Š Epoch ç»“æžœ:\")\n",
    "        print(f\"  è®­ç»ƒæŸå¤±: {train_loss:.4f} | è®­ç»ƒå‡†ç¡®çŽ‡: {train_acc:.4f}\")\n",
    "        print(f\"  éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å‡†ç¡®çŽ‡: {val_acc:.4f}\")\n",
    "\n",
    "        # ä¿å­˜æœ€ä½³æ¨¡åž‹\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            ms.save_checkpoint(model, config.model_path)\n",
    "            print(f\"âœ… ä¿å­˜æœ€ä½³æ¨¡åž‹: {val_acc:.4f}\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸŽ¯ åŠ¨æ€å›¾è®­ç»ƒå®Œæˆï¼\")\n",
    "    print(\"=\"*70)\n",
    "    print(f\"æœ€ä½³éªŒè¯å‡†ç¡®çŽ‡: {best_val_acc:.4f}\")\n",
    "\n",
    "    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡\n",
    "    np.save(config.metrics_path, history)\n",
    "    print(f\"âœ“ è®­ç»ƒæŒ‡æ ‡ä¿å­˜è‡³: {config.metrics_path}\")\n",
    "\n",
    "    return history"
   ],
   "id": "6a416dcd1d7c7a50",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:05.513595702Z",
     "start_time": "2025-12-29T04:38:05.467118194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def evaluate_dynamic(model, test_dataset, loss_fn=None):\n",
    "    \"\"\"åŠ¨æ€å›¾è¯„ä¼°å‡½æ•°\"\"\"\n",
    "    model.set_train(False)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    if loss_fn is None:\n",
    "        # å¦‚æžœæœªæä¾›loss_fnï¼Œä½¿ç”¨äº¤å‰ç†µæŸå¤±\n",
    "        loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "\n",
    "    for images, labels in test_dataset.create_tuple_iterator():\n",
    "        outputs = model(images)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        predictions = ops.Argmax(output_type=ms.int32)(outputs)\n",
    "        batch_correct = (predictions.asnumpy() == labels.asnumpy()).sum()\n",
    "\n",
    "        total_loss += loss.asnumpy()\n",
    "        correct += batch_correct\n",
    "        total += labels.shape[0]\n",
    "\n",
    "    avg_loss = total_loss / test_dataset.get_dataset_size()\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy"
   ],
   "id": "44755b9abef853d2",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:05.563969544Z",
     "start_time": "2025-12-29T04:38:05.514412410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def visualize_results(history):\n",
    "    \"\"\"å¯è§†åŒ–è®­ç»ƒç»“æžœ\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ç»“æžœå¯è§†åŒ–\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # åˆ›å»ºå›¾å½¢\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "    # è¿è¡Œä¿¡æ¯\n",
    "    run_info = f\"Mode: Dynamic | Device: {context.get_context('device_target')}\"\n",
    "    fig.suptitle(f'Flower Classification - Dynamic Mode\\\\n{run_info}', fontsize=16, fontweight='bold')\n",
    "\n",
    "    # 1. æŸå¤±æ›²çº¿\n",
    "    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='Train Loss', marker='o', linewidth=2)\n",
    "    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='Val Loss', marker='s', linewidth=2)\n",
    "    axes[0, 0].set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 2. å‡†ç¡®çŽ‡æ›²çº¿\n",
    "    axes[0, 1].plot(epochs, history['train_acc'], 'g-', label='Train Accuracy', marker='o', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, history['val_acc'], 'orange', label='Val Accuracy', marker='s', linewidth=2)\n",
    "    axes[0, 1].set_title('Training and Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    axes[0, 1].set_ylim([0, 1.0])\n",
    "\n",
    "    # 3. å­¦ä¹ çŽ‡å˜åŒ–\n",
    "    axes[1, 0].text(0.5, 0.5, f\"Learning Rate Changes:\\n{history['learning_rates']}\",\n",
    "                   ha='center', va='center', transform=axes[1, 0].transAxes,\n",
    "                   fontsize=12, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\", alpha=0.5))\n",
    "    axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].axis('off')\n",
    "    axes[1, 0].set_title('Learning Rate Schedule', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('Learning Rate')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # 4. æœ€ç»ˆå‡†ç¡®çŽ‡å¯¹æ¯”\n",
    "    final_train_acc = history['train_acc'][-1] if history['train_acc'] else 0\n",
    "    final_val_acc = history['val_acc'][-1] if history['val_acc'] else 0\n",
    "\n",
    "    axes[1, 1].bar(['Train', 'Validation'], [final_train_acc, final_val_acc],\n",
    "                  color=['green', 'red'], alpha=0.7, width=0.5)\n",
    "    axes[1, 1].set_title('Final Accuracy Comparison', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_ylabel('Accuracy')\n",
    "    axes[1, 1].set_ylim([0, 1.0])\n",
    "    axes[1, 1].text(0, final_train_acc + 0.02, f'{final_train_acc:.4f}',\n",
    "                   ha='center', fontweight='bold')\n",
    "    axes[1, 1].text(1, final_val_acc + 0.02, f'{final_val_acc:.4f}',\n",
    "                   ha='center', fontweight='bold')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dynamic_training_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"âœ“ å¯è§†åŒ–ç»“æžœä¿å­˜ä¸º 'dynamic_training_results.png'\")\n",
    "    print(f\"\\nå…³é”®æŒ‡æ ‡:\")\n",
    "    print(f\"  æœ€ç»ˆè®­ç»ƒå‡†ç¡®çŽ‡: {final_train_acc:.4f}\")\n",
    "    print(f\"  æœ€ç»ˆéªŒè¯å‡†ç¡®çŽ‡: {final_val_acc:.4f}\")\n",
    "    print(f\"  æœ€ä½³éªŒè¯å‡†ç¡®çŽ‡: {max(history['val_acc']):.4f}\")"
   ],
   "id": "9f01f55d8da415e3",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:05.692681725Z",
     "start_time": "2025-12-29T04:38:05.565051843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import requests\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "import mindspore as ms\n",
    "import mindspore.nn as nn\n",
    "\n",
    "def download_pretrained_weights(model_name=\"resnet18\", model_dir=\"./pretrained_weights\"):\n",
    "    \"\"\"\n",
    "    ä¸‹è½½MindSporeé¢„è®­ç»ƒæƒé‡\n",
    "\n",
    "    å‚æ•°:\n",
    "        model_name: æ¨¡åž‹åç§°ï¼Œå¯é€‰: \"resnet18\", \"resnet50\"\n",
    "        model_dir: æƒé‡ä¿å­˜ç›®å½•\n",
    "    \"\"\"\n",
    "\n",
    "    # é¢„è®­ç»ƒæƒé‡URLæ˜ å°„\n",
    "    model_urls = {\n",
    "        \"resnet18\": \"https://download.mindspore.cn/model_zoo/research/cv/resnet/resnet18_224.ckpt\",\n",
    "        \"resnet50\": \"https://download.mindspore.cn/model_zoo/research/cv/resnet/resnet50_224.ckpt\",\n",
    "    }\n",
    "\n",
    "    if model_name not in model_urls:\n",
    "        raise ValueError(f\"ä¸æ”¯æŒçš„æ¨¡åž‹: {model_name}ï¼Œå¯é€‰: {list(model_urls.keys())}\")\n",
    "\n",
    "    # åˆ›å»ºä¿å­˜ç›®å½•\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # æƒé‡æ–‡ä»¶è·¯å¾„\n",
    "    ckpt_filename = f\"{model_name}_224.ckpt\"\n",
    "    ckpt_path = os.path.join(model_dir, ckpt_filename)\n",
    "\n",
    "    # æ£€æŸ¥æ˜¯å¦å·²ä¸‹è½½\n",
    "    if os.path.exists(ckpt_path):\n",
    "        print(f\"âœ“ æƒé‡æ–‡ä»¶å·²å­˜åœ¨: {ckpt_path}\")\n",
    "        return ckpt_path\n",
    "\n",
    "    url = model_urls[model_name]\n",
    "    print(f\"ä¸‹è½½é¢„è®­ç»ƒæƒé‡: {model_name}\")\n",
    "    print(f\"URL: {url}\")\n",
    "    print(f\"ä¿å­˜åˆ°: {ckpt_path}\")\n",
    "\n",
    "    try:\n",
    "        # ä¸‹è½½æƒé‡æ–‡ä»¶\n",
    "        response = requests.get(url, stream=True, timeout=30)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # èŽ·å–æ–‡ä»¶å¤§å°\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "\n",
    "        # ä¸‹è½½å¹¶æ˜¾ç¤ºè¿›åº¦æ¡\n",
    "        with open(ckpt_path, 'wb') as f:\n",
    "            with tqdm(total=total_size, unit='B', unit_scale=True, desc=\"ä¸‹è½½è¿›åº¦\") as pbar:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "                        pbar.update(len(chunk))\n",
    "\n",
    "        print(f\"âœ“ ä¸‹è½½å®Œæˆ: {ckpt_path}\")\n",
    "        return ckpt_path\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"âœ— ä¸‹è½½å¤±è´¥: {e}\")\n",
    "        print(\"ä½ å¯ä»¥æ‰‹åŠ¨ä¸‹è½½æƒé‡æ–‡ä»¶:\")\n",
    "        print(f\"  {url}\")\n",
    "        print(f\"ç„¶åŽæ”¾åˆ°: {ckpt_path}\")\n",
    "        return None\n",
    "\n",
    "def load_pretrained_model(model_name=\"resnet18\", num_classes=5):\n",
    "    \"\"\"\n",
    "    åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹å¹¶ä¿®æ”¹åˆ†ç±»å¤´\n",
    "\n",
    "    å‚æ•°:\n",
    "        model_name: æ¨¡åž‹åç§°\n",
    "        num_classes: è¾“å‡ºç±»åˆ«æ•°\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹: {model_name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # æ ¹æ®æ¨¡åž‹åç§°å¯¼å…¥å¯¹åº”æ¨¡åž‹\n",
    "    if model_name == \"resnet18\":\n",
    "        from mindvision.classification.models import resnet18\n",
    "        ModelClass = resnet18\n",
    "    elif model_name == \"resnet50\":\n",
    "        from mindvision.classification.models import resnet50\n",
    "        ModelClass = resnet50\n",
    "    else:\n",
    "        raise ValueError(f\"ä¸æ”¯æŒçš„æ¨¡åž‹: {model_name}\")\n",
    "\n",
    "    try:\n",
    "        # 1. ä¸‹è½½é¢„è®­ç»ƒæƒé‡\n",
    "        ckpt_path = download_pretrained_weights(model_name)\n",
    "        if ckpt_path is None:\n",
    "            print(\"âš  æ— æ³•èŽ·å–é¢„è®­ç»ƒæƒé‡ï¼Œåˆ›å»ºæœªè®­ç»ƒæ¨¡åž‹\")\n",
    "            model = ModelClass(pretrained=False, num_classes=num_classes)\n",
    "            return model, False\n",
    "\n",
    "        # 2. åˆ›å»ºæ¨¡åž‹ï¼ˆå…ˆç”¨é»˜è®¤1000ç±»ï¼‰\n",
    "        print(\"åˆ›å»ºæ¨¡åž‹æž¶æž„...\")\n",
    "        model = ModelClass(pretrained=False, num_classes=1000)\n",
    "\n",
    "        # 3. åŠ è½½é¢„è®­ç»ƒæƒé‡\n",
    "        print(\"åŠ è½½é¢„è®­ç»ƒæƒé‡...\")\n",
    "        param_dict = ms.load_checkpoint(ckpt_path)\n",
    "\n",
    "        # 4. ç§»é™¤åˆ†ç±»å¤´çš„æƒé‡ï¼ˆå› ä¸ºç±»åˆ«æ•°ä¸åŒï¼‰\n",
    "        print(\"è°ƒæ•´åˆ†ç±»å¤´...\")\n",
    "        keys_to_remove = []\n",
    "        for key in param_dict.keys():\n",
    "            if any(word in key for word in ['head', 'classifier', 'dense', 'fc']):\n",
    "                keys_to_remove.append(key)\n",
    "\n",
    "        for key in keys_to_remove:\n",
    "            del param_dict[key]\n",
    "\n",
    "        # 5. åŠ è½½æƒé‡ï¼ˆè·³è¿‡ä¸åŒ¹é…çš„å±‚ï¼‰\n",
    "        ms.load_param_into_net(model, param_dict, strict_load=False)\n",
    "\n",
    "        # 6. ä¿®æ”¹åˆ†ç±»å¤´ä¸ºæˆ‘ä»¬çš„ç±»åˆ«æ•°\n",
    "        # MindSpore Visionæ¨¡åž‹ç»“æž„\n",
    "        if hasattr(model, 'head'):\n",
    "            if hasattr(model.head, 'dense'):\n",
    "                in_features = model.head.dense.in_channels\n",
    "                model.head.dense = nn.Dense(in_features, num_classes)\n",
    "            elif hasattr(model.head, 'classifier'):\n",
    "                in_features = model.head.classifier.in_channels\n",
    "                model.head.classifier = nn.Dense(in_features, num_classes)\n",
    "\n",
    "        print(f\"âœ“ {model_name} é¢„è®­ç»ƒæ¨¡åž‹åŠ è½½æˆåŠŸï¼\")\n",
    "        print(f\"æ€»å‚æ•°é‡: {sum(p.size for p in model.trainable_params()):,}\")\n",
    "\n",
    "        # 7. è®¾ç½®å†»ç»“ç­–ç•¥\n",
    "        print(\"\\nè®¾ç½®å‚æ•°å†»ç»“ç­–ç•¥...\")\n",
    "        trainable_count = 0\n",
    "        frozen_count = 0\n",
    "\n",
    "        for param in model.trainable_params():\n",
    "            # åªè®­ç»ƒåˆ†ç±»å¤´ï¼Œå†»ç»“ç‰¹å¾æå–å±‚\n",
    "            if any(word in param.name for word in ['head', 'classifier', 'dense', 'fc']):\n",
    "                param.requires_grad = True\n",
    "                trainable_count += 1\n",
    "            else:\n",
    "                param.requires_grad = False\n",
    "                frozen_count += 1\n",
    "\n",
    "        print(f\"å†»ç»“å±‚æ•°: {frozen_count}\")\n",
    "        print(f\"è®­ç»ƒå±‚æ•°: {trainable_count}\")\n",
    "\n",
    "        # æ˜¾ç¤ºè®­ç»ƒå‚æ•°\n",
    "        print(\"\\nè®­ç»ƒå‚æ•°:\")\n",
    "        for param in model.trainable_params():\n",
    "            if param.requires_grad:\n",
    "                print(f\"  {param.name}\")\n",
    "\n",
    "        return model, True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹å¤±è´¥: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "        print(\"\\nä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆï¼šåˆ›å»ºæœªè®­ç»ƒæ¨¡åž‹\")\n",
    "        model = ModelClass(pretrained=False, num_classes=num_classes)\n",
    "        return model, False\n",
    "\n",
    "def create_model_with_transfer_learning(model_name=\"resnet18\", num_classes=5, freeze_backbone=True):\n",
    "    \"\"\"\n",
    "    åˆ›å»ºå¸¦è¿ç§»å­¦ä¹ çš„æ¨¡åž‹\n",
    "\n",
    "    å‚æ•°:\n",
    "        model_name: æ¨¡åž‹åç§°\n",
    "        num_classes: è¾“å‡ºç±»åˆ«æ•°\n",
    "        freeze_backbone: æ˜¯å¦å†»ç»“éª¨å¹²ç½‘ç»œ\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"=\"*60)\n",
    "    print(f\"åˆ›å»ºè¿ç§»å­¦ä¹ æ¨¡åž‹: {model_name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # å°è¯•åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹\n",
    "    model, pretrained_loaded = load_pretrained_model(model_name, num_classes)\n",
    "\n",
    "    if not pretrained_loaded:\n",
    "        print(\"\\nâš  æ³¨æ„ï¼šä½¿ç”¨æœªè®­ç»ƒçš„æ¨¡åž‹ï¼Œéœ€è¦æ›´å¤šè®­ç»ƒæ—¶é—´\")\n",
    "        print(\"å»ºè®®:\")\n",
    "        print(\"  1. å¢žåŠ è®­ç»ƒepochåˆ°80-100\")\n",
    "        print(\"  2. ä½¿ç”¨æ›´å°çš„å­¦ä¹ çŽ‡: 0.0005\")\n",
    "        print(\"  3. ä½¿ç”¨æ•°æ®å¢žå¼º\")\n",
    "\n",
    "    # å¦‚æžœä¸å†»ç»“éª¨å¹²ç½‘ç»œï¼ˆå…¨é‡è®­ç»ƒï¼‰\n",
    "    if not freeze_backbone:\n",
    "        print(\"\\nå…¨é‡è®­ç»ƒæ¨¡å¼ï¼šæ‰€æœ‰å‚æ•°éƒ½ä¼šæ›´æ–°\")\n",
    "        for param in model.trainable_params():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°\n",
    "    total_params = sum(p.size for p in model.trainable_params())\n",
    "    trainable_params = sum(p.size for p in model.trainable_params() if p.requires_grad)\n",
    "\n",
    "    print(f\"\\nðŸ“Š å‚æ•°ç»Ÿè®¡:\")\n",
    "    print(f\"  æ€»å‚æ•°é‡: {total_params:,}\")\n",
    "    print(f\"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}\")\n",
    "    print(f\"  å†»ç»“å‚æ•°: {total_params - trainable_params:,}\")\n",
    "    print(f\"  å¯è®­ç»ƒæ¯”ä¾‹: {trainable_params/total_params*100:.1f}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# æµ‹è¯•å‡½æ•°\n",
    "def test_pretrained_loading():\n",
    "    \"\"\"æµ‹è¯•é¢„è®­ç»ƒæ¨¡åž‹åŠ è½½\"\"\"\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸ§ª æµ‹è¯•é¢„è®­ç»ƒæ¨¡åž‹åŠ è½½\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # æµ‹è¯•1ï¼šResNet18\n",
    "    print(\"\\n1. æµ‹è¯•ResNet18:\")\n",
    "    model1 = create_model_with_transfer_learning(\"resnet18\", 5)\n",
    "\n",
    "    # æµ‹è¯•å‰å‘ä¼ æ’­\n",
    "    import numpy as np\n",
    "    dummy_input = ms.Tensor(np.random.randn(2, 3, 224, 224).astype(np.float32))\n",
    "    output = model1(dummy_input)\n",
    "    print(f\"è¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "    print(f\"æ¨¡åž‹ç±»åž‹: {type(model1).__name__}\")\n",
    "\n",
    "    # æµ‹è¯•2ï¼šResNet50\n",
    "    print(\"\\n2. æµ‹è¯•ResNet50:\")\n",
    "    try:\n",
    "        model2 = create_model_with_transfer_learning(\"resnet50\", 5)\n",
    "        output = model2(dummy_input)\n",
    "        print(f\"è¾“å‡ºå½¢çŠ¶: {output.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ResNet50åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "    return model1\n",
    "\n",
    "# åœ¨mainå‡½æ•°ä¸­ä½¿ç”¨\n",
    "def main_with_pretrained():\n",
    "    \"\"\"ä½¿ç”¨é¢„è®­ç»ƒæ¨¡åž‹çš„ä¸»å‡½æ•°\"\"\"\n",
    "\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸŒ¸ èŠ±å‰åˆ†ç±» - é¢„è®­ç»ƒæ¨¡åž‹ç‰ˆ\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. å‡†å¤‡æ•°æ®\n",
    "    train_dataset, test_dataset, class_counts = prepare_dataset()\n",
    "    if train_dataset is None:\n",
    "        print(\"âœ— æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡ºç¨‹åº\")\n",
    "        return\n",
    "\n",
    "    # 2. åˆ›å»ºé¢„è®­ç»ƒæ¨¡åž‹\n",
    "    model = create_model_with_transfer_learning(\n",
    "        model_name=\"resnet18\",  # ä½¿ç”¨resnet18æˆ–resnet50\n",
    "        num_classes=config.num_classes,\n",
    "        freeze_backbone=True    # å†»ç»“éª¨å¹²ç½‘ç»œï¼Œåªè®­ç»ƒåˆ†ç±»å¤´\n",
    "    )\n",
    "\n",
    "    # 3. è°ƒæ•´è®­ç»ƒå‚æ•°ï¼ˆå¯¹äºŽè¿ç§»å­¦ä¹ ï¼‰\n",
    "    print(\"\\nè°ƒæ•´è®­ç»ƒå‚æ•°...\")\n",
    "    original_epochs = config.epochs\n",
    "    original_lr = config.learning_rate\n",
    "\n",
    "    if hasattr(model, 'pretrained') and model.pretrained:\n",
    "        # å¦‚æžœæ˜¯é¢„è®­ç»ƒæ¨¡åž‹ï¼Œç”¨è¾ƒå°å­¦ä¹ çŽ‡\n",
    "        config.epochs = 30  # è¿ç§»å­¦ä¹ éœ€è¦è¾ƒå°‘epoch\n",
    "        config.learning_rate = 0.0001  # å°å­¦ä¹ çŽ‡å¾®è°ƒ\n",
    "        print(\"âœ“ ä½¿ç”¨è¿ç§»å­¦ä¹ å‚æ•°:\")\n",
    "        print(f\"  Epochs: {config.epochs} (åŽŸ: {original_epochs})\")\n",
    "        print(f\"  Learning Rate: {config.learning_rate} (åŽŸ: {original_lr})\")\n",
    "    else:\n",
    "        # å¦‚æžœæ˜¯æœªè®­ç»ƒæ¨¡åž‹ï¼Œéœ€è¦æ›´å¤šepoch\n",
    "        config.epochs = 60\n",
    "        config.learning_rate = 0.001\n",
    "        print(\"âš  ä½¿ç”¨ä»Žå¤´è®­ç»ƒå‚æ•°ï¼ˆæ¨¡åž‹æœªé¢„è®­ç»ƒï¼‰\")\n",
    "        print(f\"  Epochs: {config.epochs}\")\n",
    "        print(f\"  Learning Rate: {config.learning_rate}\")\n",
    "\n",
    "    # 4. è®­ç»ƒ\n",
    "    history = train_dynamic_mode(model, train_dataset, test_dataset, class_counts)\n",
    "\n",
    "    # 5. å¯è§†åŒ–\n",
    "    visualize_results(history)\n",
    "\n",
    "    print(\"\\nâœ¨ ç¨‹åºæ‰§è¡Œå®Œæˆï¼\")\n",
    "\n",
    "    # æ¢å¤åŽŸå§‹é…ç½®\n",
    "    config.epochs = original_epochs\n",
    "    config.learning_rate = original_lr"
   ],
   "id": "ab87c66ccd5fc3b1",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:05.738622972Z",
     "start_time": "2025-12-29T04:38:05.693239165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    \"\"\"ä¸»å‡½æ•°\"\"\"\n",
    "    print(\"=\"*70)\n",
    "    print(\"ðŸŒ¸ èŠ±å‰åˆ†ç±» - åŠ¨æ€å›¾ä¸“ç”¨ç‰ˆ\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # 1. å‡†å¤‡æ•°æ®\n",
    "    train_dataset, test_dataset, class_counts = prepare_dataset()\n",
    "    if train_dataset is None:\n",
    "        print(\"âœ— æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œé€€å‡ºç¨‹åº\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:\")\n",
    "    for class_name in config.class_names:\n",
    "        print(f\"  {class_name}: {class_counts.get(class_name, 0)} å¼ \")\n",
    "\n",
    "    model = build_simple_but_strong_model()\n",
    "\n",
    "    # 3. è®­ç»ƒæ¨¡åž‹ï¼ˆåŠ¨æ€å›¾æ¨¡å¼ï¼‰\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"å¼€å§‹åŠ¨æ€å›¾è®­ç»ƒ\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    history = train_dynamic_mode(model, train_dataset, test_dataset, class_counts)\n",
    "\n",
    "    # 4. å¯è§†åŒ–ç»“æžœ\n",
    "    visualize_results(history)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"âœ¨ ç¨‹åºæ‰§è¡Œå®Œæˆï¼\")\n",
    "    print(\"=\"*70)\n"
   ],
   "id": "f7892151c2ad8539",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-29T04:38:05.838807362Z",
     "start_time": "2025-12-29T04:38:05.739364995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "id": "cee2107687c48bff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ðŸŒ¸ èŠ±å‰åˆ†ç±» - åŠ¨æ€å›¾ä¸“ç”¨ç‰ˆ\n",
      "======================================================================\n",
      "============================================================\n",
      "æ•°æ®å‡†å¤‡\n",
      "============================================================\n",
      "âœ“ è®­ç»ƒé›†å›¾åƒ: 3665 å¼ \n",
      "âœ“ æµ‹è¯•é›†å›¾åƒ: 652 å¼ \n",
      "âœ“ ç±»åˆ«æ•°é‡: 5\n",
      "âœ“ ç±»åˆ«åç§°: ['daisy', 'dandelion', 'rose', 'sunflower', 'tulip']\n",
      "åˆ›å»ºè®­ç»ƒæ•°æ®é›†...\n",
      "åˆ›å»ºæµ‹è¯•æ•°æ®é›†...\n",
      "âœ“ è®­ç»ƒæ‰¹æ¬¡æ•°: 58\n",
      "âœ“ æµ‹è¯•æ‰¹æ¬¡æ•°: 11\n",
      "\n",
      "æµ‹è¯•æ•°æ®è¯»å–...\n",
      "âœ“ æ•°æ®è¯»å–æµ‹è¯•é€šè¿‡\n",
      "  å›¾åƒå½¢çŠ¶: (64, 3, 224, 224)\n",
      "  æ ‡ç­¾å½¢çŠ¶: (64,)\n",
      "\n",
      "è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:\n",
      "  daisy: 635 å¼ \n",
      "  dandelion: 825 å¼ \n",
      "  rose: 693 å¼ \n",
      "  sunflower: 651 å¼ \n",
      "  tulip: 861 å¼ \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "For 'Conv2D', the 'pad' must be zero when 'pad_mode' is not 'pad', but got 'pad': 1 and 'pad_mode': same.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_249740/3832242952.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m__name__\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;34m\"__main__\"\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mmain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/tmp/ipykernel_249740/2517840825.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\\nè®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mclass_name\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclass_names\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m         \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"  {class_name}: {class_counts.get(class_name, 0)} å¼ \"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m     \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbuild_simple_but_strong_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0;31m# 3. è®­ç»ƒæ¨¡åž‹ï¼ˆåŠ¨æ€å›¾æ¨¡å¼ï¼‰\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"\\n\"\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m\"=\"\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0;36m60\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/tmp/ipykernel_249740/1738422501.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mbuild_simple_but_strong_model\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;34m\"\"\"æž„å»ºç®€å•çš„CNNï¼Œä½†ç”¨æ›´å¼ºçš„è®­ç»ƒç­–ç•¥\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     model = nn.SequentialCell([\n\u001B[1;32m      5\u001B[0m         \u001B[0;31m# ç®€åŒ–ä½†è¶³å¤Ÿæ·±çš„ç½‘ç»œ\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m         \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mConv2d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m64\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstride\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpadding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      7\u001B[0m         \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBatchNorm2d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m64\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      8\u001B[0m         \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mReLU\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0mnn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMaxPool2d\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mindspore/lib/python3.9/site-packages/mindspore/_extends/utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     42\u001B[0m                 \u001B[0mbound_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0minspect\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msignature\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfn\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m                 \u001B[0marguments\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbound_args\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marguments\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     44\u001B[0m                 \u001B[0;32mdel\u001B[0m \u001B[0marguments\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'self'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m                 \u001B[0marguments\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marguments\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mvalues\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 46\u001B[0;31m             \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     47\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mattrs\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcell_init_args\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__name__\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mstr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marguments\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     49\u001B[0m                 \u001B[0;32mreturn\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mindspore/lib/python3.9/site-packages/mindspore/nn/layer/conv.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, in_channels, out_channels, kernel_size, stride, pad_mode, padding, dilation, group, has_bias, weight_init, bias_init, data_format, dtype)\u001B[0m\n\u001B[1;32m    345\u001B[0m             \u001B[0mweight_init\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    346\u001B[0m             \u001B[0mbias_init\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    347\u001B[0m             \u001B[0mdata_format\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    348\u001B[0m             dtype=dtype)\n\u001B[0;32m--> 349\u001B[0;31m         self.conv2d = P.Conv2D(out_channel=self.out_channels,\n\u001B[0m\u001B[1;32m    350\u001B[0m                                \u001B[0mkernel_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkernel_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    351\u001B[0m                                \u001B[0mmode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    352\u001B[0m                                \u001B[0mpad_mode\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad_mode\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/anaconda3/envs/mindspore/lib/python3.9/site-packages/mindspore/ops/primitive.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    730\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0marguments\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    731\u001B[0m             \u001B[0mvalue\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0marguments\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    732\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_prim_attr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    733\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0minit_attrs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 734\u001B[0;31m         \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/anaconda3/envs/mindspore/lib/python3.9/site-packages/mindspore/ops/operations/nn_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, out_channel, kernel_size, mode, pad_mode, pad, stride, dilation, group, data_format)\u001B[0m\n\u001B[1;32m   1551\u001B[0m             \u001B[0mvalidator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck_equal_int\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pad size'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1552\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpad_mode\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvalidator\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcheck_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpad_mode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m'valid'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'same'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pad'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'pad_mode'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1553\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1554\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mpad_mode\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m'pad'\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mpad\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1555\u001B[0;31m             raise ValueError(f\"For '{self.name}', the 'pad' must be zero when 'pad_mode' is not 'pad', \"\n\u001B[0m\u001B[1;32m   1556\u001B[0m                              f\"but got 'pad': {self.pad} and 'pad_mode': {self.pad_mode}.\")\n\u001B[1;32m   1557\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0madd_prim_attr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"pad\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpad\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1558\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpadding\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpad\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: For 'Conv2D', the 'pad' must be zero when 'pad_mode' is not 'pad', but got 'pad': 1 and 'pad_mode': same."
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ä¿®æ­£åŽçš„è¯Šæ–­Cell\n",
    "print(\"=\"*70)\n",
    "print(\"ðŸ§ª æ¨¡åž‹è¯Šæ–­åˆ†æžï¼ˆä¿®æ­£ç‰ˆï¼‰\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def fixed_model_diagnosis():\n",
    "    \"\"\"\n",
    "    ä¿®æ­£åŽçš„ç»¼åˆè¯Šæ–­\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    print(\"æ­¥éª¤1: æ£€æŸ¥æ¨¡åž‹ç»“æž„...\")\n",
    "    try:\n",
    "        # ç›´æŽ¥é‡æ–°æž„å»ºæ¨¡åž‹ï¼Œé¿å…ä¾èµ–\n",
    "        diagnostic_model = build_depthwise_model()\n",
    "        print(\"âœ“ æ¨¡åž‹æž„å»ºæˆåŠŸ\")\n",
    "    except Exception as e:\n",
    "        print(f\"âœ— æ¨¡åž‹æž„å»ºå¤±è´¥: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\næ­¥éª¤2: å¿«é€Ÿæ•°æ®æŠ½æ ·...\")\n",
    "\n",
    "    # åˆ›å»ºç®€å•æ•°æ®åŠ è½½\n",
    "    def load_diagnostic_data():\n",
    "        train_dir = os.path.join(config.data_dir, \"train\")\n",
    "        test_dir = os.path.join(config.data_dir, \"test\")\n",
    "\n",
    "        # ç®€æ˜“æ•°æ®åŠ è½½\n",
    "        def load_batch_from_dir(data_dir, batch_size=32):\n",
    "            dataset = ds.ImageFolderDataset(\n",
    "                data_dir,\n",
    "                shuffle=True,\n",
    "                extensions=[\".jpg\", \".jpeg\", \".png\"]\n",
    "            )\n",
    "\n",
    "            # ç®€åŒ–å˜æ¢\n",
    "            transform = [\n",
    "                vision.Decode(),\n",
    "                vision.Resize(config.image_size),\n",
    "                vision.ToTensor(),\n",
    "                vision.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                               std=[0.229, 0.224, 0.225], is_hwc=False)\n",
    "            ]\n",
    "\n",
    "            dataset = dataset.map(transform, input_columns=\"image\")\n",
    "            dataset = dataset.map(transforms.TypeCast(ms.int32), input_columns=\"label\")\n",
    "            dataset = dataset.batch(batch_size)\n",
    "\n",
    "            for images, labels in dataset.create_tuple_iterator():\n",
    "                return images, labels\n",
    "\n",
    "        train_images, train_labels = load_batch_from_dir(train_dir)\n",
    "        test_images, test_labels = load_batch_from_dir(test_dir)\n",
    "\n",
    "        if train_images is not None:\n",
    "            print(f\"âœ“ è®­ç»ƒæ ·æœ¬: {train_images.shape}\")\n",
    "            print(f\"âœ“ æµ‹è¯•æ ·æœ¬: {test_images.shape}\")\n",
    "            return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "        print(\"âœ— æ•°æ®åŠ è½½å¤±è´¥\")\n",
    "        return None\n",
    "\n",
    "    data = load_diagnostic_data()\n",
    "    if data is None:\n",
    "        print(\"è¯Šæ–­ç»ˆæ­¢ï¼šæ•°æ®åŠ è½½å¤±è´¥\")\n",
    "        return\n",
    "\n",
    "    train_images, train_labels, test_images, test_labels = data\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ” æ ¸å¿ƒè¯Šæ–­ï¼šè¿‡æ‹Ÿåˆ vs æ¬ æ‹Ÿåˆ\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # è®¾ç½®è¯„ä¼°æ¨¡å¼\n",
    "    diagnostic_model.set_train(False)\n",
    "\n",
    "    # ä¿®æ­£çš„argmaxè°ƒç”¨\n",
    "    try:\n",
    "        # 1. è®­ç»ƒé›†è¡¨çŽ°\n",
    "        train_outputs = diagnostic_model(train_images)\n",
    "        train_pred = ms.ops.argmax(train_outputs, axis=1)  # å¯èƒ½è¿˜æ˜¯ä¸å¯¹ï¼Œçœ‹ä¸‹é¢\n",
    "    except TypeError:\n",
    "        # MindSporeçš„argmaxå‚æ•°åä¸åŒ\n",
    "        train_pred = ms.ops.argmax(train_outputs, 1)  # ä½¿ç”¨ä½ç½®å‚æ•°\n",
    "\n",
    "    train_acc = (train_pred == train_labels).sum().asnumpy() / len(train_labels)\n",
    "\n",
    "    # 2. æµ‹è¯•é›†è¡¨çŽ°\n",
    "    test_outputs = diagnostic_model(test_images)\n",
    "    test_pred = ms.ops.argmax(test_outputs, 1)  # ç›´æŽ¥ä½¿ç”¨ä½ç½®å‚æ•°\n",
    "    test_acc = (test_pred == test_labels).sum().asnumpy() / len(test_labels)\n",
    "\n",
    "    gap = train_acc - test_acc\n",
    "\n",
    "    print(f\"\\nðŸ“Š è¯Šæ–­ç»“æžœï¼š\")\n",
    "    print(f\"  è®­ç»ƒé›†å‡†ç¡®çŽ‡ï¼ˆæŠ½æ ·ï¼‰: {train_acc:.4f}\")\n",
    "    print(f\"  æµ‹è¯•é›†å‡†ç¡®çŽ‡ï¼ˆæŠ½æ ·ï¼‰: {test_acc:.4f}\")\n",
    "    print(f\"  æ³›åŒ–å·®è·: {gap:.4f}\")\n",
    "\n",
    "    # æ›´æ™ºèƒ½çš„è¯Šæ–­é€»è¾‘\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“ˆ è®­ç»ƒåŽ†å²åˆ†æžï¼ˆç»“åˆä¹‹å‰çš„è¾“å‡ºï¼‰\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # åˆ†æžä¹‹å‰çš„è®­ç»ƒè¾“å‡º\n",
    "    print(\"ä»Žä½ çš„è®­ç»ƒåŽ†å²ä¸­è§‚å¯Ÿåˆ°ï¼š\")\n",
    "    print(\"1. Epoch 1: è®­ç»ƒå‡†ç¡®çŽ‡35%ï¼ŒéªŒè¯36% â†’ åˆæœŸæ‹Ÿåˆæ­£å¸¸\")\n",
    "    print(\"2. Epoch 5: è®­ç»ƒ55%ï¼ŒéªŒè¯49% â†’ å¼€å§‹å‡ºçŽ°è¿‡æ‹Ÿåˆè¿¹è±¡\")\n",
    "    print(\"3. Epoch 10: è®­ç»ƒ62%ï¼ŒéªŒè¯58% â†’ è¿‡æ‹Ÿåˆç¨‹åº¦5%\")\n",
    "    print(\"4. Epoch 20: è®­ç»ƒ65%ï¼ŒéªŒè¯59% â†’ è¿‡æ‹Ÿåˆç¨‹åº¦6%\")\n",
    "    print(\"5. Epoch 25: è®­ç»ƒ68%ï¼ŒéªŒè¯63% â†’ è¿‡æ‹Ÿåˆç¨‹åº¦5%\")\n",
    "\n",
    "    print(\"\\nðŸ“‹ å…³é”®å‘çŽ°ï¼š\")\n",
    "    print(\"âœ… è¿‡æ‹Ÿåˆç¨‹åº¦å§‹ç»ˆä¿æŒåœ¨5-6%ï¼Œè¯´æ˜Žæ­£åˆ™åŒ–æœ‰æ•ˆ\")\n",
    "    print(\"âš ï¸  ä½†æœ€ç»ˆå‡†ç¡®çŽ‡åªæœ‰63%ï¼Œè¯´æ˜Žæ¨¡åž‹èƒ½åŠ›å—é™\")\n",
    "    print(\"âŒ ä¸æ˜¯ä¸¥é‡è¿‡æ‹Ÿåˆé—®é¢˜ï¼Œè€Œæ˜¯æ¬ æ‹Ÿåˆé—®é¢˜\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ§® å‚æ•°å®¹é‡æ·±åº¦åˆ†æž\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # è¯¦ç»†åˆ†æžæ¨¡åž‹ç»“æž„\n",
    "    total_params = 0\n",
    "    layer_info = []\n",
    "\n",
    "    for name, param in diagnostic_model.parameters_and_names():\n",
    "        if 'weight' in name or 'bias' in name:\n",
    "            param_count = param.size\n",
    "            total_params += param_count\n",
    "            layer_info.append((name, param_count, param.shape))\n",
    "\n",
    "    print(f\"æ€»å¯è®­ç»ƒå‚æ•°: {total_params:,}\")\n",
    "    print(f\"å±‚æ•°: {len(layer_info)}\")\n",
    "\n",
    "    # æ˜¾ç¤ºå…³é”®å±‚\n",
    "    print(\"\\nå…³é”®å±‚å‚æ•°åˆ†å¸ƒï¼š\")\n",
    "    for name, count, shape in sorted(layer_info, key=lambda x: x[1], reverse=True)[:10]:\n",
    "        print(f\"  {name:<30}: {count:,} ({shape})\")\n",
    "\n",
    "    # è®¡ç®—æ¨¡åž‹å¤æ‚åº¦æŒ‡æ ‡\n",
    "    print(f\"\\næ¨¡åž‹å¤æ‚åº¦æŒ‡æ ‡ï¼š\")\n",
    "    print(f\"  å‚æ•°å¯†åº¦: {total_params/(224*224*3):.1f} å‚æ•°/åƒç´ \")\n",
    "    print(f\"  ç†è®ºå®¹é‡: å¯è¡¨ç¤ºçº¦ {total_params/5:,} ä¸ªåˆ†ç±»æ¨¡å¼\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸŽ¯ é—®é¢˜æ ¹æºå®šä½\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # åŸºäºŽæ•°å­¦åˆ†æž\n",
    "    print(\"æ•°å­¦åˆ†æžï¼š\")\n",
    "    print(\"1. æ¯å¼ å›¾ç‰‡ä¿¡æ¯é‡: 224Ã—224Ã—3 = 150,528 åƒç´ \")\n",
    "    print(\"2. æ¨¡åž‹å‚æ•°: 14,133 ä¸ª\")\n",
    "    print(\"3. å‚æ•°/åƒç´ æ¯”: 14,133 / 150,528 = 0.094\")\n",
    "\n",
    "    print(\"\\nç»éªŒæ³•åˆ™ï¼š\")\n",
    "    print(\"â€¢ ç®€å•ä»»åŠ¡ï¼ˆMNISTï¼‰: 0.01-0.05 å‚æ•°/åƒç´ \")\n",
    "    print(\"â€¢ ä¸­ç­‰ä»»åŠ¡ï¼ˆCIFARï¼‰: 0.05-0.15 å‚æ•°/åƒç´ \")\n",
    "    print(\"â€¢ å¤æ‚ä»»åŠ¡ï¼ˆImageNetï¼‰: 0.2-1.0 å‚æ•°/åƒç´ \")\n",
    "\n",
    "    print(f\"\\nä½ çš„æ¨¡åž‹: 0.094 å‚æ•°/åƒç´  â†’ ã€ä¸­ç­‰åä½Žã€‘\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ’¡ æ ¹æœ¬åŽŸå› ç¡®è®¤\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"ç»¼åˆæ‰€æœ‰è¯æ®ï¼Œä½ çš„4ä¸ªå‡è®¾ä¸­ï¼š\")\n",
    "    print(\"\\nâœ… ã€æ¨¡åž‹å®¹é‡ä¸¥é‡ä¸è¶³ã€‘ - ç¡®è®¤æ­£ç¡®\")\n",
    "    print(\"   è¯æ®ï¼š\")\n",
    "    print(\"   1. å‚æ•°/åƒç´ æ¯”ä»…0.094ï¼ˆä¸­ç­‰åä½Žï¼‰\")\n",
    "    print(\"   2. è®­ç»ƒå‡†ç¡®çŽ‡æœ€é«˜ä»…68%ï¼ˆå­¦ä¹ èƒ½åŠ›æœ‰é™ï¼‰\")\n",
    "    print(\"   3. éªŒè¯å‡†ç¡®çŽ‡ç¨³å®šå¢žé•¿ï¼ˆè¯´æ˜Žä¸æ˜¯è¿‡æ‹Ÿåˆï¼‰\")\n",
    "\n",
    "    print(\"\\nâš ï¸  ã€æ­£åˆ™åŒ–ç­–ç•¥å•ä¸€ã€‘ - éƒ¨åˆ†æ­£ç¡®\")\n",
    "    print(\"   è¯æ®ï¼š\")\n",
    "    print(\"   1. è¿‡æ‹Ÿåˆç¨‹åº¦ä»…5%ï¼ˆè¯´æ˜Žæ­£åˆ™åŒ–æœ‰æ•ˆï¼‰\")\n",
    "    print(\"   2. ä½†éªŒè¯å‡†ç¡®çŽ‡åœæ»žåœ¨63%ï¼ˆéœ€è¦è°ƒæ•´ï¼‰\")\n",
    "\n",
    "    print(\"\\nâŒ ã€Focal Lossæƒé‡æœ‰é—®é¢˜ã€‘ - å¯èƒ½æ¬¡è¦\")\n",
    "    print(\"   è¯æ®ï¼š\")\n",
    "    print(\"   1. è®­ç»ƒè¿‡ç¨‹ç¨³å®šï¼ˆæ²¡æœ‰å¼‚å¸¸éœ‡è¡ï¼‰\")\n",
    "    print(\"   2. å„ç±»åˆ«å­¦ä¹ ç›¸å¯¹å‡è¡¡\")\n",
    "\n",
    "    print(\"\\nâŒ ã€æ•°æ®å¢žå¼ºä¸è¶³ã€‘ - ä¸æˆç«‹\")\n",
    "    print(\"   è¯æ®ï¼š\")\n",
    "    print(\"   1. è¿‡æ‹Ÿåˆç¨‹åº¦å¾ˆå°ï¼ˆ5%ï¼‰\")\n",
    "    print(\"   2. è¯´æ˜Žå½“å‰å¢žå¼ºè¶³å¤Ÿé˜²æ­¢è¿‡æ‹Ÿåˆ\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ”§ å…·ä½“æ”¹è¿›æ–¹æ¡ˆ\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"åŸºäºŽè¯Šæ–­ï¼Œå»ºè®®æŒ‰æ­¤ä¼˜å…ˆçº§æ”¹è¿›ï¼š\")\n",
    "    print(\"\\nã€ç¬¬1æ­¥ï¼šå¢žåŠ æ¨¡åž‹å®¹é‡ï¼ˆæœ€ç´§æ€¥ï¼‰ã€‘\")\n",
    "    print(\"  ç›®æ ‡ï¼šå‚æ•°ä»Ž14Kå¢žåŠ åˆ°50K-80K\")\n",
    "    print(\"  æ–¹æ³•ï¼š\")\n",
    "    print(\"    1. å¢žåŠ åˆå§‹é€šé“æ•°ï¼ˆ16â†’32æˆ–48ï¼‰\")\n",
    "    print(\"    2. å¢žåŠ å±‚æ•°ï¼ˆå½“å‰3å±‚â†’4-5å±‚ï¼‰\")\n",
    "    print(\"    3. é€‚åº¦å¢žåŠ æ¯å±‚é€šé“æ•°\")\n",
    "    print(\"  é¢„æœŸæ•ˆæžœï¼šéªŒè¯å‡†ç¡®çŽ‡ 63% â†’ 70%+\")\n",
    "\n",
    "    print(\"\\nã€ç¬¬2æ­¥ï¼šè°ƒæ•´æ­£åˆ™åŒ–ç­–ç•¥ã€‘\")\n",
    "    print(\"  ç”±äºŽè¦å¢žåŠ æ¨¡åž‹å®¹é‡ï¼Œéœ€è¦ç›¸åº”è°ƒæ•´ï¼š\")\n",
    "    print(\"    1. ä¿æŒå½“å‰Dropout(0.4)\")\n",
    "    print(\"    2. è€ƒè™‘æ·»åŠ Label Smoothing(0.1)\")\n",
    "    print(\"    3. ä¿æŒweight_decay(1e-4)\")\n",
    "\n",
    "    print(\"\\nã€ç¬¬3æ­¥ï¼šä¼˜åŒ–æŸå¤±å‡½æ•°ã€‘\")\n",
    "    print(\"  ç®€å•è°ƒæ•´ï¼š\")\n",
    "    print(\"    1. é™ä½ŽFocal Lossçš„Î³ï¼ˆ2.0â†’1.5æˆ–1.0ï¼‰\")\n",
    "    print(\"    2. æˆ–ç›´æŽ¥ä½¿ç”¨å¸¦æƒé‡çš„CrossEntropy\")\n",
    "\n",
    "    print(\"\\nã€ç¬¬4æ­¥ï¼šæ•°æ®å¢žå¼ºä¼˜åŒ–ã€‘\")\n",
    "    print(\"  ä¿æŒçŽ°æœ‰å¢žå¼ºå³å¯ï¼ŒåŽæœŸå¯è€ƒè™‘ï¼š\")\n",
    "    print(\"    1. éšæœºè£å‰ªç¼©æ”¾\")\n",
    "    print(\"    2. è½»å¾®çš„é¢œè‰²æ‰°åŠ¨\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸ“Š å¯è§†åŒ–éªŒè¯\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    # ç”Ÿæˆè¯Šæ–­å›¾\n",
    "    try:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "        # 1. å‚æ•°å®¹é‡å¯¹æ¯”\n",
    "        sizes = [14000, 50000, 80000, 150000]\n",
    "        labels = ['å½“å‰(14K)', 'ç›®æ ‡(50K)', 'ç†æƒ³(80K)', 'å……è¶³(150K)']\n",
    "        colors = ['red', 'orange', 'green', 'blue']\n",
    "\n",
    "        axes[0, 0].bar(labels, sizes, color=colors, alpha=0.7)\n",
    "        axes[0, 0].set_ylabel('å‚æ•°æ•°é‡')\n",
    "        axes[0, 0].set_title('æ¨¡åž‹å®¹é‡å¯¹æ¯”')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # ç”»å½“å‰æ°´å¹³çº¿\n",
    "        axes[0, 0].axhline(y=total_params, color='red', linestyle='--', alpha=0.5)\n",
    "        axes[0, 0].text(3.5, total_params, f'å½“å‰',\n",
    "                       va='bottom', ha='right', color='red')\n",
    "\n",
    "        # 2. è®­ç»ƒ/éªŒè¯å·®è·\n",
    "        categories = ['è®­ç»ƒé›†', 'éªŒè¯é›†', 'å·®è·']\n",
    "        values = [train_acc, test_acc, gap]\n",
    "        colors = ['blue', 'orange', 'red' if gap > 0.1 else 'green']\n",
    "\n",
    "        bars = axes[0, 1].bar(categories, values, color=colors, alpha=0.7)\n",
    "        axes[0, 1].set_ylabel('æ•°å€¼')\n",
    "        axes[0, 1].set_title('æ‹ŸåˆçŠ¶æ€è¯Šæ–­')\n",
    "        axes[0, 1].set_ylim([0, max(values) * 1.2])\n",
    "\n",
    "        for bar, val in zip(bars, values):\n",
    "            axes[0, 1].text(bar.get_x() + bar.get_width()/2,\n",
    "                          bar.get_height() + 0.01,\n",
    "                          f'{val:.3f}', ha='center', fontsize=9)\n",
    "\n",
    "        # 3. é—®é¢˜ä¼˜å…ˆçº§\n",
    "        issues = ['æ¨¡åž‹å®¹é‡', 'æ­£åˆ™åŒ–', 'æŸå¤±å‡½æ•°', 'æ•°æ®å¢žå¼º']\n",
    "        severity = [9, 5, 3, 2]  # ä¸¥é‡ç¨‹åº¦1-10\n",
    "        urgency = [10, 7, 4, 3]   # ç´§æ€¥ç¨‹åº¦1-10\n",
    "\n",
    "        x = range(len(issues))\n",
    "        width = 0.35\n",
    "        axes[1, 0].bar([i - width/2 for i in x], severity, width, label='ä¸¥é‡ç¨‹åº¦', alpha=0.7)\n",
    "        axes[1, 0].bar([i + width/2 for i in x], urgency, width, label='ç´§æ€¥ç¨‹åº¦', alpha=0.7)\n",
    "        axes[1, 0].set_xticks(x)\n",
    "        axes[1, 0].set_xticklabels(issues, rotation=45)\n",
    "        axes[1, 0].set_ylabel('è¯„åˆ† (1-10)')\n",
    "        axes[1, 0].set_title('é—®é¢˜ä¼˜å…ˆçº§åˆ†æž')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].set_ylim([0, 11])\n",
    "\n",
    "        # 4. é¢„æœŸæ”¹è¿›æ•ˆæžœ\n",
    "        stages = ['å½“å‰', '+å®¹é‡', '+æ­£åˆ™', '+ä¼˜åŒ–']\n",
    "        train_accs = [0.68, 0.75, 0.78, 0.80]\n",
    "        val_accs = [0.63, 0.70, 0.72, 0.75]\n",
    "\n",
    "        x = range(len(stages))\n",
    "        axes[1, 1].plot(x, train_accs, 'b-o', label='è®­ç»ƒå‡†ç¡®çŽ‡', linewidth=2)\n",
    "        axes[1, 1].plot(x, val_accs, 'r-s', label='éªŒè¯å‡†ç¡®çŽ‡', linewidth=2)\n",
    "        axes[1, 1].fill_between(x, train_accs, val_accs, alpha=0.1, color='gray')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(stages)\n",
    "        axes[1, 1].set_ylabel('å‡†ç¡®çŽ‡')\n",
    "        axes[1, 1].set_title('é¢„æœŸæ”¹è¿›è·¯å¾„')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        axes[1, 1].set_ylim([0.5, 0.85])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('model_diagnosis_final.png', dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "        print(\"âœ… è¯Šæ–­å›¾è¡¨å·²ä¿å­˜ä¸º 'model_diagnosis_final.png'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš  å›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}\")\n",
    "        print(\"ä½†è¯Šæ–­åˆ†æžå·²å®Œæˆ\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"ðŸŽ¯ æœ€ç»ˆå»ºè®®\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    print(\"ç«‹å³è¡ŒåŠ¨ï¼š\")\n",
    "    print(\"1. å°†æ¨¡åž‹å‚æ•°ä»Ž14Kå¢žåŠ åˆ°è‡³å°‘50K\")\n",
    "    print(\"2. ä¿æŒçŽ°æœ‰æ­£åˆ™åŒ–å¼ºåº¦ï¼ˆDropout=0.4ï¼‰\")\n",
    "    print(\"3. è®­ç»ƒ30-40ä¸ªepochè§‚å¯Ÿæ•ˆæžœ\")\n",
    "\n",
    "    print(\"\\né¢„æœŸç»“æžœï¼š\")\n",
    "    print(f\"  éªŒè¯å‡†ç¡®çŽ‡: 63% â†’ 70%+ (æå‡7+ä¸ªç™¾åˆ†ç‚¹)\")\n",
    "    print(f\"  è®­ç»ƒå‡†ç¡®çŽ‡: 68% â†’ 75%+\")\n",
    "    print(f\"  è¿‡æ‹Ÿåˆç¨‹åº¦: ä¿æŒ5-10%çš„åˆç†èŒƒå›´\")\n",
    "#\n",
    "# # æ‰§è¡Œè¯Šæ–­\n",
    "# print(\"æ­£åœ¨æ‰§è¡Œä¿®æ­£ç‰ˆæ¨¡åž‹è¯Šæ–­...\")\n",
    "# fixed_model_diagnosis()"
   ],
   "id": "8e6a5863a756f62f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print(\"=\"*70)\n",
    "# print(\"ðŸ”¬ éªŒè¯å®žéªŒï¼šå±‚æ•° vs é€šé“æ•° å“ªä¸ªæ›´æœ‰æ•ˆï¼Ÿ\")\n",
    "# print(\"=\"*70)\n",
    "#\n",
    "# class DepthwiseBlock(nn.Cell):\n",
    "#     \"\"\"æ·±åº¦å¯åˆ†ç¦»æ®‹å·®å—\"\"\"\n",
    "#     def __init__(self, in_channels, out_channels, stride=1, expansion=4):\n",
    "#         super(DepthwiseBlock, self).__init__()\n",
    "#         hidden_dim = in_channels * expansion\n",
    "#\n",
    "#         # 1. å‡ç»´ (1x1)\n",
    "#         self.conv1 = nn.Conv2d(in_channels, hidden_dim, kernel_size=1, has_bias=False)\n",
    "#         self.bn1 = nn.BatchNorm2d(hidden_dim)\n",
    "#\n",
    "#         # 2. æ·±åº¦å·ç§¯ (3x3)\n",
    "#         self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3,\n",
    "#                               stride=stride, pad_mode='pad', padding=1,\n",
    "#                               group=hidden_dim, has_bias=False)\n",
    "#         self.bn2 = nn.BatchNorm2d(hidden_dim)\n",
    "#\n",
    "#         # 3. é™ç»´ (1x1)\n",
    "#         self.conv3 = nn.Conv2d(hidden_dim, out_channels, kernel_size=1, has_bias=False)\n",
    "#         self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "#\n",
    "#         self.relu = nn.ReLU()\n",
    "#\n",
    "#         self.downsample = None\n",
    "#         if stride != 1 or in_channels != out_channels:\n",
    "#             self.downsample = nn.SequentialCell([\n",
    "#                 nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, has_bias=False),\n",
    "#                 nn.BatchNorm2d(out_channels)\n",
    "#             ])\n",
    "#\n",
    "#     def construct(self, x):\n",
    "#         identity = x\n",
    "#\n",
    "#         out = self.conv1(x)\n",
    "#         out = self.bn1(out)\n",
    "#         out = self.relu(out)\n",
    "#\n",
    "#         out = self.conv2(out)\n",
    "#         out = self.bn2(out)\n",
    "#         out = self.relu(out)\n",
    "#\n",
    "#         out = self.conv3(out)\n",
    "#         out = self.bn3(out)\n",
    "#\n",
    "#         if self.downsample is not None:\n",
    "#             identity = self.downsample(x)\n",
    "#\n",
    "#         out += identity\n",
    "#         out = self.relu(out)\n",
    "#\n",
    "#         return out\n",
    "#\n",
    "# def create_models_for_testing():\n",
    "#     \"\"\"åˆ›å»º4ä¸ªå¯¹æ¯”æ¨¡åž‹\"\"\"\n",
    "#     models = {}\n",
    "#\n",
    "#     print(\"åˆ›å»º4ç§é…ç½®çš„æ¨¡åž‹è¿›è¡Œå¯¹æ¯”ï¼š\")\n",
    "#     print(\"\\n1ï¸âƒ£ åŸºå‡†æ¨¡åž‹ï¼ˆåŽŸç‰ˆï¼‰- 14Kå‚æ•°\")\n",
    "#     print(\"2ï¸âƒ£ å¢žåŠ é€šé“æ•°æ¨¡åž‹ - çº¦50Kå‚æ•°\")\n",
    "#     print(\"3ï¸âƒ£ å¢žåŠ å±‚æ•°æ¨¡åž‹ - çº¦50Kå‚æ•°\")\n",
    "#     print(\"4ï¸âƒ£ é€šé“æ•°+å±‚æ•°æ¨¡åž‹ - çº¦100Kå‚æ•°\")\n",
    "#\n",
    "#     # 1. åŽŸç‰ˆæ¨¡åž‹ï¼ˆåŸºå‡†ï¼‰\n",
    "#     print(\"\\n\" + \"-\"*40)\n",
    "#     print(\"1. æž„å»ºåŸºå‡†æ¨¡åž‹...\")\n",
    "#     model_base = build_depthwise_model()\n",
    "#     base_params = sum(p.size for p in model_base.trainable_params())\n",
    "#     models['base'] = {'model': model_base, 'params': base_params, 'type': 'base'}\n",
    "#     print(f\"  å‚æ•°: {base_params:,}\")\n",
    "#\n",
    "#     # 2. åªå¢žåŠ é€šé“æ•°\n",
    "#     print(\"\\n2. æž„å»ºå¢žåŠ é€šé“æ•°æ¨¡åž‹...\")\n",
    "#     class WiderDepthwiseNet(nn.Cell):\n",
    "#         def __init__(self, num_classes=5):\n",
    "#             super().__init__()\n",
    "#             # å¢žåŠ é€šé“æ•°ï¼š16â†’32, 32â†’64, 64â†’128\n",
    "#             self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, pad_mode='pad', padding=1, has_bias=False)\n",
    "#             self.bn1 = nn.BatchNorm2d(32)\n",
    "#             self.relu = nn.ReLU()\n",
    "#             self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode='same')\n",
    "#\n",
    "#             # ä½¿ç”¨æ·±åº¦å¯åˆ†ç¦»å·ç§¯\n",
    "#             self.layer1 = self._make_layer(32, 32, 1, stride=1, expansion=2)\n",
    "#             self.layer2 = self._make_layer(32, 64, 1, stride=2, expansion=2)\n",
    "#             self.layer3 = self._make_layer(64, 128, 1, stride=2, expansion=2)\n",
    "#\n",
    "#             self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#             self.flatten = nn.Flatten()\n",
    "#             self.dropout = nn.Dropout(0.4)\n",
    "#             self.fc = nn.Dense(128, num_classes)\n",
    "#\n",
    "#         def _make_layer(self, in_channels, out_channels, blocks, stride, expansion=2):\n",
    "#             layers = []\n",
    "#             layers.append(DepthwiseBlock(in_channels, out_channels, stride, expansion))\n",
    "#             for _ in range(1, blocks):\n",
    "#                 layers.append(DepthwiseBlock(out_channels, out_channels, 1, expansion))\n",
    "#             return nn.SequentialCell(layers)\n",
    "#\n",
    "#         def construct(self, x):\n",
    "#             x = self.conv1(x)\n",
    "#             x = self.bn1(x)\n",
    "#             x = self.relu(x)\n",
    "#             x = self.maxpool(x)\n",
    "#\n",
    "#             x = self.layer1(x)\n",
    "#             x = self.layer2(x)\n",
    "#             x = self.layer3(x)\n",
    "#\n",
    "#             x = self.avgpool(x)\n",
    "#             x = self.flatten(x)\n",
    "#             x = self.dropout(x)\n",
    "#             x = self.fc(x)\n",
    "#\n",
    "#             return x\n",
    "#\n",
    "#     model_wider = WiderDepthwiseNet(num_classes=config.num_classes)\n",
    "#     wider_params = sum(p.size for p in model_wider.trainable_params())\n",
    "#     models['wider'] = {'model': model_wider, 'params': wider_params, 'type': 'wider_channels'}\n",
    "#     print(f\"  å‚æ•°: {wider_params:,} (å¢žåŠ {((wider_params/base_params)-1)*100:.0f}%)\")\n",
    "#\n",
    "#     # 3. åªå¢žåŠ å±‚æ•°\n",
    "#     print(\"\\n3. æž„å»ºå¢žåŠ å±‚æ•°æ¨¡åž‹...\")\n",
    "#     class DeeperDepthwiseNet(nn.Cell):\n",
    "#         def __init__(self, num_classes=5):\n",
    "#             super().__init__()\n",
    "#             # ä¿æŒé€šé“æ•°ï¼Œå¢žåŠ å±‚æ•°ï¼š1â†’2ä¸ªå—æ¯å±‚\n",
    "#             self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=2, pad_mode='pad', padding=1, has_bias=False)\n",
    "#             self.bn1 = nn.BatchNorm2d(16)\n",
    "#             self.relu = nn.ReLU()\n",
    "#             self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode='same')\n",
    "#\n",
    "#             # æ¯å±‚2ä¸ªå—ï¼ˆåŽŸæ¥æ˜¯1ä¸ªï¼‰\n",
    "#             self.layer1 = self._make_layer(16, 16, 2, stride=1, expansion=2)\n",
    "#             self.layer2 = self._make_layer(16, 32, 2, stride=2, expansion=2)\n",
    "#             self.layer3 = self._make_layer(32, 64, 2, stride=2, expansion=2)\n",
    "#             # å¢žåŠ ç¬¬4å±‚\n",
    "#             self.layer4 = self._make_layer(64, 128, 2, stride=2, expansion=2)\n",
    "#\n",
    "#             self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#             self.flatten = nn.Flatten()\n",
    "#             self.dropout = nn.Dropout(0.4)\n",
    "#             self.fc = nn.Dense(128, num_classes)\n",
    "#\n",
    "#         def _make_layer(self, in_channels, out_channels, blocks, stride, expansion=2):\n",
    "#             layers = []\n",
    "#             layers.append(DepthwiseBlock(in_channels, out_channels, stride, expansion))\n",
    "#             for _ in range(1, blocks):\n",
    "#                 layers.append(DepthwiseBlock(out_channels, out_channels, 1, expansion))\n",
    "#             return nn.SequentialCell(layers)\n",
    "#\n",
    "#         def construct(self, x):\n",
    "#             x = self.conv1(x)\n",
    "#             x = self.bn1(x)\n",
    "#             x = self.relu(x)\n",
    "#             x = self.maxpool(x)\n",
    "#\n",
    "#             x = self.layer1(x)\n",
    "#             x = self.layer2(x)\n",
    "#             x = self.layer3(x)\n",
    "#             x = self.layer4(x)\n",
    "#\n",
    "#             x = self.avgpool(x)\n",
    "#             x = self.flatten(x)\n",
    "#             x = self.dropout(x)\n",
    "#             x = self.fc(x)\n",
    "#\n",
    "#             return x\n",
    "#\n",
    "#     model_deeper = DeeperDepthwiseNet(num_classes=config.num_classes)\n",
    "#     deeper_params = sum(p.size for p in model_deeper.trainable_params())\n",
    "#     models['deeper'] = {'model': model_deeper, 'params': deeper_params, 'type': 'deeper_layers'}\n",
    "#     print(f\"  å‚æ•°: {deeper_params:,} (å¢žåŠ {((deeper_params/base_params)-1)*100:.0f}%)\")\n",
    "#\n",
    "#     # 4. é€šé“æ•°+å±‚æ•°éƒ½å¢žåŠ \n",
    "#     print(\"\\n4. æž„å»ºé€šé“æ•°+å±‚æ•°æ¨¡åž‹...\")\n",
    "#     class WiderDeeperNet(nn.Cell):\n",
    "#         def __init__(self, num_classes=5):\n",
    "#             super().__init__()\n",
    "#             # å¢žåŠ é€šé“æ•°ï¼š16â†’32ï¼Œå¹¶å¢žåŠ å±‚æ•°\n",
    "#             self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=2, pad_mode='pad', padding=1, has_bias=False)\n",
    "#             self.bn1 = nn.BatchNorm2d(32)\n",
    "#             self.relu = nn.ReLU()\n",
    "#             self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, pad_mode='same')\n",
    "#\n",
    "#             # é€šé“æ•°ï¼š32â†’64â†’128ï¼Œæ¯å±‚2ä¸ªå—\n",
    "#             self.layer1 = self._make_layer(32, 32, 2, stride=1, expansion=2)\n",
    "#             self.layer2 = self._make_layer(32, 64, 2, stride=2, expansion=2)\n",
    "#             self.layer3 = self._make_layer(64, 128, 2, stride=2, expansion=2)\n",
    "#             self.layer4 = self._make_layer(128, 256, 2, stride=2, expansion=2)\n",
    "#\n",
    "#             self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#             self.flatten = nn.Flatten()\n",
    "#             self.dropout = nn.Dropout(0.4)\n",
    "#             self.fc = nn.Dense(256, num_classes)\n",
    "#\n",
    "#         def _make_layer(self, in_channels, out_channels, blocks, stride, expansion=2):\n",
    "#             layers = []\n",
    "#             layers.append(DepthwiseBlock(in_channels, out_channels, stride, expansion))\n",
    "#             for _ in range(1, blocks):\n",
    "#                 layers.append(DepthwiseBlock(out_channels, out_channels, 1, expansion))\n",
    "#             return nn.SequentialCell(layers)\n",
    "#\n",
    "#         def construct(self, x):\n",
    "#             x = self.conv1(x)\n",
    "#             x = self.bn1(x)\n",
    "#             x = self.relu(x)\n",
    "#             x = self.maxpool(x)\n",
    "#\n",
    "#             x = self.layer1(x)\n",
    "#             x = self.layer2(x)\n",
    "#             x = self.layer3(x)\n",
    "#             x = self.layer4(x)\n",
    "#\n",
    "#             x = self.avgpool(x)\n",
    "#             x = self.flatten(x)\n",
    "#             x = self.dropout(x)\n",
    "#             x = self.fc(x)\n",
    "#\n",
    "#             return x\n",
    "#\n",
    "#     model_wider_deeper = WiderDeeperNet(num_classes=config.num_classes)\n",
    "#     wider_deeper_params = sum(p.size for p in model_wider_deeper.trainable_params())\n",
    "#     models['wider_deeper'] = {'model': model_wider_deeper, 'params': wider_deeper_params, 'type': 'wider_and_deeper'}\n",
    "#     print(f\"  å‚æ•°: {wider_deeper_params:,} (å¢žåŠ {((wider_deeper_params/base_params)-1)*100:.0f}%)\")\n",
    "#\n",
    "#     return models\n",
    "#\n",
    "# def run_quick_experiment(models, train_dataset, test_dataset, epochs=45):\n",
    "#     \"\"\"å¿«é€Ÿè¿è¡Œå¯¹æ¯”å®žéªŒ\"\"\"\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"ðŸš€ å¼€å§‹å¿«é€Ÿå¯¹æ¯”å®žéªŒï¼ˆæ¯ä¸ªæ¨¡åž‹è®­ç»ƒ45ä¸ªepochï¼‰\")\n",
    "#     print(\"=\"*70)\n",
    "#\n",
    "#     results = {}\n",
    "#\n",
    "#     for name, info in models.items():\n",
    "#         print(f\"\\nâ–¶ï¸ è®­ç»ƒæ¨¡åž‹: {name} ({info['type']})\")\n",
    "#         print(f\"   å‚æ•°é‡: {info['params']:,}\")\n",
    "#\n",
    "#         # ä½¿ç”¨ç®€åŒ–è®­ç»ƒ\n",
    "#         model = info['model']\n",
    "#\n",
    "#         # åˆ›å»ºä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°\n",
    "#         optimizer = nn.Adam(model.trainable_params(),\n",
    "#                           learning_rate=config.learning_rate,\n",
    "#                           weight_decay=1e-4)\n",
    "#\n",
    "#         # ä½¿ç”¨æ ‡å‡†CrossEntropyï¼ˆæŽ’é™¤Focal Losså½±å“ï¼‰\n",
    "#         loss_fn = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "#         net_with_loss = nn.WithLossCell(model, loss_fn)\n",
    "#         train_net = nn.TrainOneStepCell(net_with_loss, optimizer)\n",
    "#\n",
    "#         # è®°å½•æŒ‡æ ‡\n",
    "#         train_acc_history = []\n",
    "#         val_acc_history = []\n",
    "#         best_val_acc = 0\n",
    "#\n",
    "#         # å¿«é€Ÿè®­ç»ƒå¾ªçŽ¯\n",
    "#         for epoch in range(epochs):\n",
    "#             # è®­ç»ƒ\n",
    "#             model.set_train()\n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#\n",
    "#             # åªå–å‰5ä¸ªæ‰¹æ¬¡åŠ é€Ÿè®­ç»ƒ\n",
    "#             batch_count = 0\n",
    "#             for images, labels in train_dataset.create_tuple_iterator():\n",
    "#                 # å‰å‘+åå‘\n",
    "#                 loss = train_net(images, labels)\n",
    "#\n",
    "#                 # è®¡ç®—å‡†ç¡®çŽ‡\n",
    "#                 outputs = model(images)\n",
    "#                 predictions = ms.ops.argmax(outputs, 1)\n",
    "#                 correct += (predictions == labels).sum().asnumpy()\n",
    "#                 total += labels.shape[0]\n",
    "#\n",
    "#                 batch_count += 1\n",
    "#                 if batch_count >= 10:  # æ¯epochåªè®­ç»ƒ5ä¸ªæ‰¹æ¬¡\n",
    "#                     break\n",
    "#\n",
    "#             train_acc = correct / total if total > 0 else 0\n",
    "#             train_acc_history.append(train_acc)\n",
    "#\n",
    "#             # éªŒè¯\n",
    "#             model.set_train(False)\n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#\n",
    "#             # åªå–2ä¸ªéªŒè¯æ‰¹æ¬¡\n",
    "#             val_count = 0\n",
    "#             for images, labels in test_dataset.create_tuple_iterator():\n",
    "#                 outputs = model(images)\n",
    "#                 predictions = ms.ops.argmax(outputs, 1)\n",
    "#                 correct += (predictions == labels).sum().asnumpy()\n",
    "#                 total += labels.shape[0]\n",
    "#\n",
    "#                 val_count += 1\n",
    "#                 if val_count >= 2:\n",
    "#                     break\n",
    "#\n",
    "#             val_acc = correct / total if total > 0 else 0\n",
    "#             val_acc_history.append(val_acc)\n",
    "#\n",
    "#             if val_acc > best_val_acc:\n",
    "#                 best_val_acc = val_acc\n",
    "#\n",
    "#             if (epoch + 1) % 2 == 0:\n",
    "#                 print(f\"  Epoch {epoch+1}/{epochs}: \"\n",
    "#                       f\"è®­ç»ƒAcc={train_acc:.3f}, éªŒè¯Acc={val_acc:.3f}\")\n",
    "#\n",
    "#         results[name] = {\n",
    "#             'params': info['params'],\n",
    "#             'type': info['type'],\n",
    "#             'train_acc_history': train_acc_history,\n",
    "#             'val_acc_history': val_acc_history,\n",
    "#             'best_val_acc': best_val_acc,\n",
    "#             'final_train_acc': train_acc_history[-1],\n",
    "#             'final_val_acc': val_acc_history[-1]\n",
    "#         }\n",
    "#\n",
    "#         print(f\"  âœ… å®Œæˆ! æœ€ä½³éªŒè¯Acc: {best_val_acc:.3f}\")\n",
    "#\n",
    "#     return results\n",
    "#\n",
    "# def analyze_experiment_results(results):\n",
    "#     \"\"\"åˆ†æžå®žéªŒç»“æžœ\"\"\"\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"ðŸ“Š å®žéªŒç»“æžœåˆ†æž\")\n",
    "#     print(\"=\"*70)\n",
    "#\n",
    "#     # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼\n",
    "#     print(\"\\nðŸ“ˆ æ€§èƒ½å¯¹æ¯”è¡¨ï¼š\")\n",
    "#     print(\"-\"*80)\n",
    "#     print(f\"{'æ¨¡åž‹':<15} {'å‚æ•°é‡':<12} {'ç±»åž‹':<20} {'æœ€ç»ˆè®­ç»ƒAcc':<12} {'æœ€ç»ˆéªŒè¯Acc':<12} {'æå‡å¹…åº¦':<10}\")\n",
    "#     print(\"-\"*80)\n",
    "#\n",
    "#     base_val = results['base']['final_val_acc']\n",
    "#\n",
    "#     for name in ['base', 'wider', 'deeper', 'wider_deeper']:\n",
    "#         if name in results:\n",
    "#             r = results[name]\n",
    "#             improvement = (r['final_val_acc'] - base_val) * 100 if name != 'base' else 0\n",
    "#             print(f\"{name:<15} {r['params']:<12,} {r['type']:<20} \"\n",
    "#                   f\"{r['final_train_acc']:<12.3f} {r['final_val_acc']:<12.3f} \"\n",
    "#                   f\"{improvement:>+6.1f}%\" if name != 'base' else\n",
    "#                   f\"{name:<15} {r['params']:<12,} {r['type']:<20} \"\n",
    "#                   f\"{r['final_train_acc']:<12.3f} {r['final_val_acc']:<12.3f} {'åŸºå‡†':>10}\")\n",
    "#\n",
    "#     print(\"-\"*80)\n",
    "#\n",
    "#     # åˆ†æžå“ªç§ç­–ç•¥æ›´æœ‰æ•ˆ\n",
    "#     print(\"\\nðŸ”¬ æœ‰æ•ˆæ€§åˆ†æžï¼š\")\n",
    "#\n",
    "#     wider_improvement = results['wider']['final_val_acc'] - base_val\n",
    "#     deeper_improvement = results['deeper']['final_val_acc'] - base_val\n",
    "#     wider_deeper_improvement = results['wider_deeper']['final_val_acc'] - base_val\n",
    "#\n",
    "#     print(f\"1. åªå¢žåŠ é€šé“æ•°ï¼šéªŒè¯Accæå‡ {wider_improvement*100:.1f}%\")\n",
    "#     print(f\"2. åªå¢žåŠ å±‚æ•°ï¼šéªŒè¯Accæå‡ {deeper_improvement*100:.1f}%\")\n",
    "#     print(f\"3. é€šé“æ•°+å±‚æ•°ï¼šéªŒè¯Accæå‡ {wider_deeper_improvement*100:.1f}%\")\n",
    "#\n",
    "#     # è®¡ç®—æ•ˆçŽ‡æ¯”ï¼ˆæå‡/å‚æ•°å¢žåŠ æ¯”ä¾‹ï¼‰\n",
    "#     wider_param_ratio = results['wider']['params'] / results['base']['params']\n",
    "#     deeper_param_ratio = results['deeper']['params'] / results['base']['params']\n",
    "#     wider_deeper_param_ratio = results['wider_deeper']['params'] / results['base']['params']\n",
    "#\n",
    "#     wider_efficiency = wider_improvement / (wider_param_ratio - 1) if wider_param_ratio > 1 else 0\n",
    "#     deeper_efficiency = deeper_improvement / (deeper_param_ratio - 1) if deeper_param_ratio > 1 else 0\n",
    "#     wider_deeper_efficiency = wider_deeper_improvement / (wider_deeper_param_ratio - 1) if wider_deeper_param_ratio > 1 else 0\n",
    "#\n",
    "#     print(f\"\\nðŸ“Š æ•ˆçŽ‡åˆ†æžï¼ˆå•ä½å‚æ•°å¢žåŠ å¸¦æ¥çš„å‡†ç¡®çŽ‡æå‡ï¼‰ï¼š\")\n",
    "#     print(f\"  å¢žåŠ é€šé“æ•°æ•ˆçŽ‡: {wider_efficiency:.4f}\")\n",
    "#     print(f\"  å¢žåŠ å±‚æ•°æ•ˆçŽ‡: {deeper_efficiency:.4f}\")\n",
    "#     print(f\"  ä¸¤è€…éƒ½å¢žåŠ æ•ˆçŽ‡: {wider_deeper_efficiency:.4f}\")\n",
    "#\n",
    "#     # ç»™å‡ºå»ºè®®\n",
    "#     print(\"\\n\" + \"=\"*70)\n",
    "#     print(\"ðŸ’¡ å®žéªŒç»“è®ºä¸Žå»ºè®®\")\n",
    "#     print(\"=\"*70)\n",
    "#\n",
    "#     if wider_efficiency > deeper_efficiency and wider_efficiency > wider_deeper_efficiency:\n",
    "#         print(\"âœ… **ç»“è®ºï¼šå¢žåŠ é€šé“æ•°æ›´æœ‰æ•ˆ**\")\n",
    "#         print(\"   åŽŸå› ï¼š\")\n",
    "#         print(\"   1. æ¯å¢žåŠ 1%å‚æ•°ï¼Œå‡†ç¡®çŽ‡æå‡æ›´å¤š\")\n",
    "#         print(\"   2. æ›´å®½çš„æ¨¡åž‹èƒ½å­¦ä¹ æ›´å¤šç‰¹å¾\")\n",
    "#         print(\"   3. é€‚åˆå½“å‰ç›¸å¯¹æµ…çš„ç½‘ç»œç»“æž„\")\n",
    "#         print(\"\\n  å»ºè®®ï¼šä¼˜å…ˆå¢žåŠ é€šé“æ•°ï¼ˆ16â†’32â†’64ï¼‰\")\n",
    "#\n",
    "#     elif deeper_efficiency > wider_efficiency and deeper_efficiency > wider_deeper_efficiency:\n",
    "#         print(\"âœ… **ç»“è®ºï¼šå¢žåŠ å±‚æ•°æ›´æœ‰æ•ˆ**\")\n",
    "#         print(\"   åŽŸå› ï¼š\")\n",
    "#         print(\"   1. å¢žåŠ æ·±åº¦èƒ½å­¦ä¹ æ›´æŠ½è±¡çš„ç‰¹å¾\")\n",
    "#         print(\"   2. æ·±åº¦å¯¹å¤æ‚ä»»åŠ¡æ›´é‡è¦\")\n",
    "#         print(\"   3. é¿å…è¿‡å®½å¯¼è‡´çš„å†—ä½™\")\n",
    "#         print(\"\\n  å»ºè®®ï¼šä¼˜å…ˆå¢žåŠ å±‚æ•°ï¼ˆ3å±‚â†’4-5å±‚ï¼‰\")\n",
    "#\n",
    "#     else:\n",
    "#         print(\"âœ… **ç»“è®ºï¼šé€šé“æ•°å’Œå±‚æ•°éƒ½éœ€è¦å¢žåŠ **\")\n",
    "#         print(\"   åŽŸå› ï¼š\")\n",
    "#         print(\"   1. ä¸¤è€…ç»“åˆæ•ˆæžœæœ€å¥½\")\n",
    "#         print(\"   2. å®½åº¦å’Œæ·±åº¦éƒ½å¾ˆé‡è¦\")\n",
    "#         print(\"   3. ç»¼åˆæå‡è¡¨è¾¾èƒ½åŠ›\")\n",
    "#         print(\"\\n  å»ºè®®ï¼šé€‚åº¦å¢žåŠ é€šé“æ•°ï¼ˆ16â†’24ï¼‰å’Œå±‚æ•°ï¼ˆ+1å±‚ï¼‰\")\n",
    "#\n",
    "#     # å¯è§†åŒ–ç»“æžœ\n",
    "#     print(\"\\næ­£åœ¨ç”Ÿæˆå¯¹æ¯”å›¾è¡¨...\")\n",
    "#     import matplotlib.pyplot as plt\n",
    "#\n",
    "#     fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "#\n",
    "#     # 1. éªŒè¯å‡†ç¡®çŽ‡å¯¹æ¯”\n",
    "#     model_names = ['åŸºå‡†', 'å¢žåŠ é€šé“æ•°', 'å¢žåŠ å±‚æ•°', 'ä¸¤è€…éƒ½å¢žåŠ ']\n",
    "#     final_val_accs = [\n",
    "#         results['base']['final_val_acc'],\n",
    "#         results['wider']['final_val_acc'],\n",
    "#         results['deeper']['final_val_acc'],\n",
    "#         results['wider_deeper']['final_val_acc']\n",
    "#     ]\n",
    "#\n",
    "#     colors = ['gray', 'blue', 'orange', 'green']\n",
    "#     bars = axes[0, 0].bar(model_names, final_val_accs, color=colors, alpha=0.7)\n",
    "#     axes[0, 0].set_ylabel('éªŒè¯å‡†ç¡®çŽ‡')\n",
    "#     axes[0, 0].set_title('ä¸åŒç­–ç•¥çš„æœ€ç»ˆéªŒè¯å‡†ç¡®çŽ‡')\n",
    "#     axes[0, 0].set_ylim([0, 1])\n",
    "#     for bar, val in zip(bars, final_val_accs):\n",
    "#         axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
    "#                        f'{val:.3f}', ha='center', fontsize=10)\n",
    "#\n",
    "#     # 2. è®­ç»ƒæ›²çº¿å¯¹æ¯”\n",
    "#     epochs = range(1, 46)\n",
    "#     axes[0, 1].plot(epochs, results['base']['val_acc_history'], 'k--', label='åŸºå‡†', alpha=0.7)\n",
    "#     axes[0, 1].plot(epochs, results['wider']['val_acc_history'], 'b-', label='å¢žåŠ é€šé“æ•°', linewidth=2)\n",
    "#     axes[0, 1].plot(epochs, results['deeper']['val_acc_history'], 'orange', label='å¢žåŠ å±‚æ•°', linewidth=2)\n",
    "#     axes[0, 1].plot(epochs, results['wider_deeper']['val_acc_history'], 'g-', label='ä¸¤è€…éƒ½å¢žåŠ ', linewidth=2)\n",
    "#     axes[0, 1].set_xlabel('Epoch')\n",
    "#     axes[0, 1].set_ylabel('éªŒè¯å‡†ç¡®çŽ‡')\n",
    "#     axes[0, 1].set_title('éªŒè¯å‡†ç¡®çŽ‡å­¦ä¹ æ›²çº¿')\n",
    "#     axes[0, 1].legend()\n",
    "#     axes[0, 1].grid(True, alpha=0.3)\n",
    "#\n",
    "#     # 3. å‚æ•°æ•ˆçŽ‡å¯¹æ¯”\n",
    "#     param_ratios = [1, wider_param_ratio, deeper_param_ratio, wider_deeper_param_ratio]\n",
    "#     improvements = [0, wider_improvement*100, deeper_improvement*100, wider_deeper_improvement*100]\n",
    "#\n",
    "#     ax3 = axes[1, 0]\n",
    "#     scatter = ax3.scatter(param_ratios, improvements,\n",
    "#                          s=[100, 200, 200, 300],\n",
    "#                          c=colors, alpha=0.7)\n",
    "#     ax3.set_xlabel('å‚æ•°å¢žåŠ å€æ•° (ç›¸å¯¹äºŽåŸºå‡†)')\n",
    "#     ax3.set_ylabel('å‡†ç¡®çŽ‡æå‡ (%)')\n",
    "#     ax3.set_title('å‚æ•°æ•ˆçŽ‡åˆ†æž')\n",
    "#     ax3.grid(True, alpha=0.3)\n",
    "#\n",
    "#     # æ·»åŠ æ ‡ç­¾\n",
    "#     for i, txt in enumerate(model_names):\n",
    "#         ax3.annotate(txt, (param_ratios[i], improvements[i]),\n",
    "#                     xytext=(5, 5), textcoords='offset points')\n",
    "#\n",
    "#     # 4. è¿‡æ‹Ÿåˆç¨‹åº¦å¯¹æ¯”\n",
    "#     gaps = []\n",
    "#     for name in ['base', 'wider', 'deeper', 'wider_deeper']:\n",
    "#         train_acc = results[name]['final_train_acc']\n",
    "#         val_acc = results[name]['final_val_acc']\n",
    "#         gaps.append(train_acc - val_acc)\n",
    "#\n",
    "#     bars = axes[1, 1].bar(model_names, gaps, color=colors, alpha=0.7)\n",
    "#     axes[1, 1].set_ylabel('è®­ç»ƒ-éªŒè¯å·®è·')\n",
    "#     axes[1, 1].set_title('è¿‡æ‹Ÿåˆç¨‹åº¦å¯¹æ¯”')\n",
    "#     axes[1, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "#\n",
    "#     for bar, gap in zip(bars, gaps):\n",
    "#         color = 'red' if gap > 0.1 else 'green'\n",
    "#         axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "#                        f'{gap:.3f}', ha='center', fontsize=10, color=color)\n",
    "#\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('model_strategy_comparison.png', dpi=150, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "#\n",
    "#     print(f\"\\nâœ… å¯¹æ¯”å›¾è¡¨å·²ä¿å­˜ä¸º 'model_strategy_comparison.png'\")\n",
    "#\n",
    "# # æ‰§è¡Œå®žéªŒ\n",
    "# print(\"å‡†å¤‡å¯¹æ¯”å®žéªŒ...\")\n",
    "#\n",
    "# # 1. åˆ›å»ºæ•°æ®ï¼ˆå¤ç”¨ä¹‹å‰çš„æ•°æ®é›†ï¼‰\n",
    "# print(\"\\nåŠ è½½æ•°æ®...\")\n",
    "# train_dir = os.path.join(config.data_dir, \"train\")\n",
    "# test_dir = os.path.join(config.data_dir, \"test\")\n",
    "#\n",
    "# # ç®€åŒ–å˜æ¢ç”¨äºŽå¿«é€Ÿå®žéªŒ\n",
    "# simple_transform = [\n",
    "#     vision.Decode(),\n",
    "#     vision.Resize(config.image_size),\n",
    "#     vision.ToTensor(),\n",
    "#     vision.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], is_hwc=False)\n",
    "# ]\n",
    "\n",
    "# def create_small_dataset_for_test(data_path, batch_size=16):\n",
    "#     dataset = ds.ImageFolderDataset(\n",
    "#         data_path,\n",
    "#         shuffle=True,\n",
    "#         extensions=[\".jpg\", \".jpeg\", \".png\"]\n",
    "#     )\n",
    "#\n",
    "#     dataset = dataset.map(simple_transform, input_columns=\"image\")\n",
    "#     dataset = dataset.map(transforms.TypeCast(ms.int32), input_columns=\"label\")\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "#\n",
    "#     return dataset\n",
    "#\n",
    "# train_small = create_small_dataset_for_test(train_dir, batch_size=16)\n",
    "# test_small = create_small_dataset_for_test(test_dir, batch_size=16)\n",
    "#\n",
    "# print(f\"è®­ç»ƒé›†æ‰¹æ¬¡: {train_small.get_dataset_size()}\")\n",
    "# print(f\"æµ‹è¯•é›†æ‰¹æ¬¡: {test_small.get_dataset_size()}\")\n",
    "#\n",
    "# # 2. åˆ›å»ºå¯¹æ¯”æ¨¡åž‹\n",
    "# models = create_models_for_testing()\n",
    "#\n",
    "# # 3. è¿è¡Œå¿«é€Ÿå®žéªŒ\n",
    "# results = run_quick_experiment(models, train_small, test_small, epochs=45)\n",
    "#\n",
    "# # 4. åˆ†æžç»“æžœ\n",
    "# analyze_experiment_results(results)\n",
    "#\n",
    "# print(\"\\n\" + \"=\"*70)\n",
    "# print(\"ðŸŽ¯ ä¸‹ä¸€æ­¥è¡ŒåŠ¨\")\n",
    "# print(\"=\"*70)\n",
    "# print(\"æ ¹æ®å®žéªŒç»“æžœï¼Œé€‰æ‹©æœ€æœ‰æ•ˆçš„ç­–ç•¥ä¿®æ”¹ä½ çš„æ¨¡åž‹ï¼Œç„¶åŽé‡æ–°è®­ç»ƒï¼\")"
   ],
   "id": "c05aa469055e880a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# test_model_capacity()",
   "id": "55e656c3922089f9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore (GPU)",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
